name: Test Report

on:
  workflow_run:
    workflows: ["CI", "Comprehensive CI"]
    types:
      - completed

permissions:
  contents: read
  checks: write
  pull-requests: write
  actions: read

jobs:
  report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: ${{ github.event.workflow_run.conclusion != 'skipped' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6
      
      - name: Wait for artifacts (up to 30s)
        run: |
          echo "Waiting for artifacts to be available..."
          for i in {1..6}; do
            echo "Attempt $i/6..."
            sleep 5
          done
      
      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          path: test-artifacts
          merge-multiple: true
        continue-on-error: true
      
      - name: Check for artifacts
        id: check_artifacts
        run: |
          if [ -d "test-artifacts" ] && [ -n "$(ls -A test-artifacts 2>/dev/null)" ]; then
            echo "artifacts_found=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Artifacts found:"
            find test-artifacts -type f
          else
            echo "artifacts_found=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è  No artifacts found"
          fi
      
      - name: Publish test results
        if: steps.check_artifacts.outputs.artifacts_found == 'true'
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: |
            test-artifacts/**/*.xml
          check_name: Test Results (${{ github.event.workflow_run.name }})
          comment_title: Test Results
          comment_mode: always
          compare_to_earlier_commit: true
          report_individual_runs: true
          report_suite_logs: error
        continue-on-error: true
      
      - name: Set up Python for analysis
        if: steps.check_artifacts.outputs.artifacts_found == 'true'
        uses: actions/setup-python@v6
        with:
          python-version: '3.10'
      
      - name: Analyze test results
        if: steps.check_artifacts.outputs.artifacts_found == 'true'
        run: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path
          from datetime import datetime
          
          def analyze_json_reports():
              """Analyze JSON test reports."""
              reports = list(Path('test-artifacts').glob('**/*.json'))
              
              if not reports:
                  print("No JSON test reports found")
                  return None
              
              total_tests = 0
              total_passed = 0
              total_failed = 0
              total_skipped = 0
              total_duration = 0.0
              failures = []
              
              for report_path in reports:
                  try:
                      with open(report_path) as f:
                          data = json.load(f)
                      
                      summary = data.get('summary', {})
                      total_tests += summary.get('total', 0)
                      total_passed += summary.get('passed', 0)
                      total_failed += summary.get('failed', 0)
                      total_skipped += summary.get('skipped', 0)
                      total_duration += data.get('duration', 0.0)
                      
                      # Collect failures
                      for test in data.get('tests', []):
                          if test.get('outcome') == 'failed':
                              failures.append({
                                  'name': test.get('nodeid', 'unknown'),
                                  'message': test.get('call', {}).get('longrepr', 'No message')
                              })
                  except Exception as e:
                      print(f"Error reading {report_path}: {e}")
              
              return {
                  'total': total_tests,
                  'passed': total_passed,
                  'failed': total_failed,
                  'skipped': total_skipped,
                  'duration': total_duration,
                  'failures': failures
              }
          
          # Analyze reports
          results = analyze_json_reports()
          
          if results:
              # Create summary
              with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
                  f.write("## üìä Test Results Summary\n\n")
                  
                  # Overall stats
                  f.write("### Overall Statistics\n\n")
                  f.write(f"- **Total Tests:** {results['total']}\n")
                  f.write(f"- **‚úÖ Passed:** {results['passed']}\n")
                  f.write(f"- **‚ùå Failed:** {results['failed']}\n")
                  f.write(f"- **‚è≠Ô∏è  Skipped:** {results['skipped']}\n")
                  f.write(f"- **‚è±Ô∏è  Duration:** {results['duration']:.2f}s\n\n")
                  
                  # Pass rate
                  if results['total'] > 0:
                      pass_rate = (results['passed'] / results['total']) * 100
                      f.write(f"**Pass Rate:** {pass_rate:.1f}%\n\n")
                  
                  # Failures detail
                  if results['failures']:
                      f.write("### ‚ùå Failed Tests\n\n")
                      for i, failure in enumerate(results['failures'][:10], 1):
                          f.write(f"{i}. `{failure['name']}`\n")
                          # Truncate long error messages
                          msg = failure['message'][:200]
                          if len(failure['message']) > 200:
                              msg += "..."
                          f.write(f"   ```\n   {msg}\n   ```\n\n")
                      
                      if len(results['failures']) > 10:
                          f.write(f"*... and {len(results['failures']) - 10} more failures*\n\n")
                  else:
                      f.write("### ‚úÖ All Tests Passed!\n\n")
                  
                  # Status emoji
                  if results['failed'] == 0:
                      f.write("## üéâ Success!\n")
                  elif results['failed'] < 5:
                      f.write("## ‚ö†Ô∏è  Minor Issues\n")
                  else:
                      f.write("## üö® Attention Required\n")
          else:
              with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
                  f.write("## ‚ÑπÔ∏è  No JSON Test Reports\n\n")
                  f.write("JSON test reports not found. ")
                  f.write("Check XML reports for basic test counts.\n")
          
          EOF
        continue-on-error: true
      
      - name: No artifacts found message
        if: steps.check_artifacts.outputs.artifacts_found == 'false'
        run: |
          echo "## ‚ÑπÔ∏è  No Test Artifacts Found" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Test artifacts were not available. This can happen if:" >> $GITHUB_STEP_SUMMARY
          echo "- Tests didn't run in the triggering workflow" >> $GITHUB_STEP_SUMMARY
          echo "- Tests are still running" >> $GITHUB_STEP_SUMMARY
          echo "- Artifacts failed to upload" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Check the [workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}) for details." >> $GITHUB_STEP_SUMMARY
      
      - name: Upload combined report
        if: always() && steps.check_artifacts.outputs.artifacts_found == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: combined-test-report-${{ github.event.workflow_run.id }}
          path: test-artifacts/
          retention-days: 90
        continue-on-error: true
