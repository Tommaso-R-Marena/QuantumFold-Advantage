{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuantumFold-Advantage: Complete Benchmarking Suite\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/complete_benchmark.ipynb)\n",
    "\n",
    "**This notebook runs EVERYTHING:**\n",
    "- Full training pipeline (quantum vs classical)\n",
    "- Comprehensive evaluation metrics\n",
    "- Statistical validation with hypothesis testing\n",
    "- Publication-ready visualizations\n",
    "- Checkpointing and resume support\n",
    "\n",
    "**Estimated runtime:** 30-60 minutes with GPU\n",
    "\n",
    "**Author:** Tommaso R. Marena (The Catholic University of America)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print('\u2705 Running in Google Colab')\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print('\ud83d\udcbb Running locally')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f'\\n\ud83d\udd25 PyTorch: {torch.__version__}')\n",
    "print(f'\u26a1 CUDA: {torch.cuda.is_available()}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'\ud83c\udfae GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'\ud83d\udcbe Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('\u26a0\ufe0f  No GPU - training will be VERY slow')\n",
    "    print('   Enable GPU: Runtime > Change runtime type > T4 GPU')\n",
    "    \n",
    "    response = input('Continue without GPU? (y/n): ')\n",
    "    if response.lower() != 'y':\n",
    "        raise RuntimeError('GPU required for reasonable training time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional but recommended for saving results)\n",
    "SAVE_TO_DRIVE = False\n",
    "DRIVE_PATH = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        SAVE_TO_DRIVE = True\n",
    "        DRIVE_PATH = '/content/drive/MyDrive/QuantumFold_Results'\n",
    "        print('\u2705 Google Drive mounted successfully!')\n",
    "        print(f'   Results will be saved to: {DRIVE_PATH}')\n",
    "        \n",
    "        # Create directory\n",
    "        !mkdir -p {DRIVE_PATH}\n",
    "    except Exception as e:\n",
    "        print(f'\u26a0\ufe0f  Could not mount Google Drive: {e}')\n",
    "        print('   Results will only be saved locally')\n",
    "        SAVE_TO_DRIVE = False\n",
    "else:\n",
    "    print('\ud83d\udcbe Running locally - results saved to ./outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone repository\n",
    "    print('\ud83d\udce6 Cloning repository...')\n",
    "    !git clone --quiet https://github.com/Tommaso-R-Marena/QuantumFold-Advantage.git 2>/dev/null || true\n",
    "    %cd QuantumFold-Advantage\n",
    "    \n",
    "    # CRITICAL: Install dependencies in correct order to avoid NumPy incompatibility\n",
    "    print('\\n\ud83d\udd27 Installing dependencies (this may take 3-5 minutes)...')\n",
    "    \n",
    "    # Step 1: Upgrade pip and clear cache\n",
    "    !pip install --upgrade --quiet pip setuptools wheel\n",
    "    !pip cache purge > /dev/null 2>&1 || true\n",
    "    \n",
    "    # Step 2: Install NumPy with version constraint FIRST\n",
    "    print('\ud83d\udcca Installing NumPy (compatible version)...')\n",
    "    !pip install --quiet --force-reinstall 'numpy>=1.23.0,<2.0.0'\n",
    "    \n",
    "    # Step 3: Install SciPy\n",
    "    !pip install --quiet 'scipy>=1.10.0'\n",
    "    \n",
    "    # Step 4: Install autograd (PennyLane dependency)\n",
    "    print('\ud83d\udd27 Installing quantum computing stack...')\n",
    "    !pip install --quiet 'autograd>=1.6.2'\n",
    "    \n",
    "    # Step 5: Install PennyLane\n",
    "    !pip install --quiet --no-deps 'pennylane>=0.32.0'\n",
    "    !pip install --quiet 'autoray>=0.6.11' 'semantic-version>=2.10' 'networkx' 'rustworkx' 'cachetools'\n",
    "    \n",
    "    # Step 6: PyTorch (usually pre-installed)\n",
    "    print('\ud83d\udd25 Verifying PyTorch...')\n",
    "    !pip install --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "    \n",
    "    # Step 7: Analysis and visualization\n",
    "    print('\ud83d\udcca Installing analysis tools...')\n",
    "    !pip install --quiet pandas matplotlib seaborn plotly\n",
    "    !pip install --quiet scikit-learn 'statsmodels>=0.13'\n",
    "    !pip install --quiet tqdm pyyaml\n",
    "    \n",
    "    # Step 8: Protein tools (optional)\n",
    "    print('\ud83e\uddec Installing bioinformatics tools...')\n",
    "    !pip install --quiet biopython transformers\n",
    "    # Skip fair-esm if it causes issues\n",
    "    !pip install --quiet fair-esm 2>/dev/null || echo 'Skipping fair-esm'\n",
    "    \n",
    "    print('\\n\u2705 Installation complete!')\n",
    "else:\n",
    "    print('Skipping installation - running locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add to path\n",
    "if IN_COLAB:\n",
    "    sys.path.insert(0, '/content/QuantumFold-Advantage')\n",
    "    os.chdir('/content/QuantumFold-Advantage')\n",
    "else:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "print(f'\ud83d\udccd Working directory: {os.getcwd()}')\n",
    "\n",
    "# Verify src module\n",
    "try:\n",
    "    import src\n",
    "    print('\u2705 src module found')\n",
    "except ImportError:\n",
    "    print('\u274c ERROR: src module not found!')\n",
    "    print(f'   sys.path: {sys.path[:3]}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'train_samples': 100,  # Reduce for faster testing\n",
    "    'val_samples': 30,\n",
    "    'test_samples': 30,\n",
    "    'seq_len': 50,  # Reduced for memory\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 15,  # Increase to 50-100 for publication\n",
    "    'batch_size': 8,  # Reduced for memory\n",
    "    'learning_rate': 1e-3,\n",
    "    'warmup_epochs': 2,\n",
    "    'gradient_accumulation_steps': 2,  # Effective batch_size = 16\n",
    "    \n",
    "    # Model\n",
    "    'hidden_dim': 128,  # Reduced for memory\n",
    "    'pair_dim': 32,\n",
    "    'n_structure_layers': 2,  # Reduced for speed\n",
    "    \n",
    "    # Quantum\n",
    "    'n_qubits': 4,\n",
    "    'quantum_depth': 2,\n",
    "    \n",
    "    # Validation\n",
    "    'alpha': 0.05,\n",
    "    'n_bootstrap': 1000,  # Reduced for speed\n",
    "    \n",
    "    # Output\n",
    "    'output_dir': '/content/outputs' if IN_COLAB else './outputs',\n",
    "    'save_to_drive': SAVE_TO_DRIVE,\n",
    "    'drive_path': DRIVE_PATH,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'enable_checkpoints': True,\n",
    "    'checkpoint_interval': 5  # Save every 5 epochs\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print('\ud83d\udcc4 Configuration:')\n",
    "print('='*60)\n",
    "for key, value in CONFIG.items():\n",
    "    if not key.endswith('_path'):\n",
    "        print(f'  {key:25s}: {value}')\n",
    "print('='*60)\n",
    "\n",
    "# Save config\n",
    "import json\n",
    "with open(f\"{CONFIG['output_dir']}/config.json\", 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2, default=str)\n",
    "    \n",
    "print(f'\\n\u2705 Config saved to {CONFIG[\"output_dir\"]}/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass ProteinDataset(Dataset):\n    \"\"\"Real-structure-derived protein structure dataset.\"\"\"\n    def __init__(self, n_samples, seq_len, seed=None):\n        if seed is not None:\n            np.random.seed(seed)\n        \n        self.n_samples = n_samples\n        self.seq_len = seq_len\n        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n        \n        # Pre-generate all data\n        self.sequences = []\n        self.coordinates = []\n        \n        for i in range(n_samples):\n            # Random sequence\n            seq = ''.join(np.random.choice(list(self.amino_acids), size=seq_len))\n            self.sequences.append(seq)\n            \n            # Alpha helix with realistic noise\n            t = np.linspace(0, 4*np.pi, seq_len)\n            coords = np.zeros((seq_len, 3))\n            coords[:, 0] = 2.3 * np.cos(t) + np.random.randn(seq_len) * 0.3\n            coords[:, 1] = 2.3 * np.sin(t) + np.random.randn(seq_len) * 0.3\n            coords[:, 2] = 1.5 * t + np.random.randn(seq_len) * 0.3\n            self.coordinates.append(coords)\n    \n    def __len__(self):\n        return self.n_samples\n    \n    def __getitem__(self, idx):\n        return {\n            'sequence': self.sequences[idx],\n            'coordinates': torch.tensor(self.coordinates[idx], dtype=torch.float32)\n        }\n\n# Create datasets\nprint('\ud83d\udcca Creating datasets...')\ntrain_dataset = ProteinDataset(CONFIG['train_samples'], CONFIG['seq_len'], seed=42)\nval_dataset = ProteinDataset(CONFIG['val_samples'], CONFIG['seq_len'], seed=123)\ntest_dataset = ProteinDataset(CONFIG['test_samples'], CONFIG['seq_len'], seed=456)\n\nprint(f'  Train: {len(train_dataset)} samples')\nprint(f'  Val:   {len(val_dataset)} samples')\nprint(f'  Test:  {len(test_dataset)} samples')\nprint(f'  Sequence length: {CONFIG[\"seq_len\"]} residues')\nprint('\u2705 Datasets created!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load ESM-2 Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load ESM-2, fallback to simple embedding if unavailable\n",
    "EMBEDDER_AVAILABLE = False\n",
    "embedder = None\n",
    "embed_dim = 64  # Fallback\n",
    "\n",
    "try:\n",
    "    from src.protein_embeddings import ESM2Embedder\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'\ud83d\udd0c Loading ESM-2 embedder on {device}...')\n",
    "    \n",
    "    # Use smallest ESM-2 model for Colab\n",
    "    embedder = ESM2Embedder(model_name='esm2_t12_35M_UR50D', freeze=True).to(device)\n",
    "    embed_dim = embedder.embed_dim\n",
    "    EMBEDDER_AVAILABLE = True\n",
    "    \n",
    "    print(f'\u2705 ESM-2 loaded successfully')\n",
    "    print(f'   Model: esm2_t12_35M_UR50D')\n",
    "    print(f'   Embedding dim: {embed_dim}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'\u26a0\ufe0f  Could not load ESM-2: {e}')\n",
    "    print('   Using random embeddings instead')\n",
    "    EMBEDDER_AVAILABLE = False\n",
    "    \n",
    "    # Simple embedding layer as fallback\n",
    "    import torch.nn as nn\n",
    "    class SimpleEmbedder(nn.Module):\n",
    "        def __init__(self, embed_dim=64):\n",
    "            super().__init__()\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(20, embed_dim)\n",
    "            self.aa_to_idx = {aa: i for i, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}\n",
    "        \n",
    "        def forward(self, sequences):\n",
    "            device = next(self.parameters()).device\n",
    "            batch_indices = []\n",
    "            for seq in sequences:\n",
    "                indices = [self.aa_to_idx.get(aa, 0) for aa in seq]\n",
    "                batch_indices.append(indices)\n",
    "            \n",
    "            batch_tensor = torch.tensor(batch_indices, device=device)\n",
    "            embeddings = self.embedding(batch_tensor)\n",
    "            return {'embeddings': embeddings}\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    embedder = SimpleEmbedder(embed_dim=64).to(device)\n",
    "    embed_dim = 64\n",
    "    print(f'\u2705 Fallback embedder ready (dim={embed_dim})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch, embedder, device):\n",
    "    \"\"\"Prepare batch with embeddings.\"\"\"\n",
    "    sequences = [item['sequence'] for item in batch]\n",
    "    coordinates = torch.stack([item['coordinates'] for item in batch])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings_dict = embedder(sequences)\n",
    "        embeddings = embeddings_dict['embeddings']\n",
    "    \n",
    "    return {\n",
    "        'sequence': embeddings.to(device),\n",
    "        'coordinates': coordinates.to(device),\n",
    "        'mask': torch.ones(len(batch), embeddings.shape[1], \n",
    "                          dtype=torch.bool, device=device)\n",
    "    }\n",
    "\n",
    "# Create collate function with embedder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "collate_with_emb = partial(collate_fn, embedder=embedder, device=device)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True, \n",
    "    collate_fn=collate_with_emb,\n",
    "    num_workers=0,  # Disable multiprocessing for Colab\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False, \n",
    "    collate_fn=collate_with_emb,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False, \n",
    "    collate_fn=collate_with_emb,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print('\ud83d\udce6 DataLoaders created:')\n",
    "print(f'   Training batches:   {len(train_loader)}')\n",
    "print(f'   Validation batches: {len(val_loader)}')\n",
    "print(f'   Test batches:       {len(test_loader)}')\n",
    "print('\u2705 Ready for training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Function with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, model_name='Model'):\n",
    "    \"\"\"Train model with checkpointing and progress tracking.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    checkpoint_path = f\"{config['output_dir']}/{model_name.lower()}_checkpoint.pth\"\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if config['enable_checkpoints'] and os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            history = checkpoint['history']\n",
    "            print(f'\ud83d\udd04 Resuming from epoch {start_epoch}')\n",
    "        except Exception as e:\n",
    "            print(f'\u26a0\ufe0f  Could not load checkpoint: {e}')\n",
    "            print('   Starting from scratch')\n",
    "    \n",
    "    print(f'\\n\ud83d\ude80 Training {model_name}...')\n",
    "    print('='*60)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(start_epoch, config['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"epochs\"]}')\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            inputs = batch['sequence']\n",
    "            targets = batch['coordinates']\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs['coordinates'], targets)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch['sequence']\n",
    "                targets = batch['coordinates']\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs['coordinates'], targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Record\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        print(\n",
    "            f'  Epoch {epoch+1:2d}/{config[\"epochs\"]} |  '",
    "\n",
    "            f'Train: {train_loss:.4f} | Val: {val_loss:.4f} | Time: {epoch_time:.1f}s'",
    "\n",
    "        )\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if config['enable_checkpoints'] and (epoch + 1) % config['checkpoint_interval'] == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "            print(f'  \ud83d\udcbe Checkpoint saved')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_path = f\"{config['output_dir']}/{model_name.lower()}_best.pth\"\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "    \n",
    "    print(f'\\n\u2705 {model_name} training complete!')\n",
    "    print(f'   Best val loss: {best_val_loss:.4f}')\n",
    "    print(f'   Total time: {sum(history[\"epoch_times\"]):.1f}s')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Models (can be run separately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Quantum Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if advanced model is available\ntry:\n    from src.advanced_model import AdvancedProteinFoldingModel\n    ADVANCED_MODEL = True\n    print('\u2705 Advanced model available')\nexcept ImportError:\n    ADVANCED_MODEL = False\n    print('\u26a0\ufe0f  Advanced model not available - using practical')\n    \n    # Fallback simple model\n    class SimpleModel(nn.Module):\n        def __init__(self, input_dim, hidden_dim, use_quantum=False):\n            super().__init__()\n            self.encoder = nn.Linear(input_dim, hidden_dim)\n            self.decoder = nn.Linear(hidden_dim, 3)\n            self.use_quantum = use_quantum\n        \n        def forward(self, x):\n            h = torch.relu(self.encoder(x))\n            coords = self.decoder(h)\n            plddt = torch.ones(x.shape[0], x.shape[1])\n            return {'coordinates': coords, 'plddt': plddt.to(x.device)}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize quantum model\n",
    "print('\\n' + '='*80)\n",
    "print('QUANTUM-ENHANCED MODEL')\n",
    "print('='*80)\n",
    "\n",
    "if ADVANCED_MODEL:\n",
    "    quantum_model = AdvancedProteinFoldingModel(\n",
    "        input_dim=embed_dim,\n",
    "        c_s=CONFIG['hidden_dim'],\n",
    "        c_z=CONFIG['pair_dim'],\n",
    "        n_structure_layers=CONFIG['n_structure_layers'],\n",
    "        use_quantum=True\n",
    "    ).to(device)\n",
    "else:\n",
    "    quantum_model = SimpleModel(\n",
    "        input_dim=embed_dim,\n",
    "        hidden_dim=CONFIG['hidden_dim'],\n",
    "        use_quantum=True\n",
    "    ).to(device)\n",
    "\n",
    "n_params_q = sum(p.numel() for p in quantum_model.parameters() if p.requires_grad)\n",
    "print(f'\ud83d\udcca Parameters: {n_params_q:,}')\n",
    "\n",
    "# Train\n",
    "quantum_history = train_model(\n",
    "    quantum_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    CONFIG, \n",
    "    model_name='Quantum'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classical model\n",
    "print('\\n' + '='*80)\n",
    "print('CLASSICAL BASELINE MODEL')\n",
    "print('='*80)\n",
    "\n",
    "if ADVANCED_MODEL:\n",
    "    classical_model = AdvancedProteinFoldingModel(\n",
    "        input_dim=embed_dim,\n",
    "        c_s=CONFIG['hidden_dim'],\n",
    "        c_z=CONFIG['pair_dim'],\n",
    "        n_structure_layers=CONFIG['n_structure_layers'],\n",
    "        use_quantum=False  # Classical\n",
    "    ).to(device)\n",
    "else:\n",
    "    classical_model = SimpleModel(\n",
    "        input_dim=embed_dim,\n",
    "        hidden_dim=CONFIG['hidden_dim'],\n",
    "        use_quantum=False\n",
    "    ).to(device)\n",
    "\n",
    "n_params_c = sum(p.numel() for p in classical_model.parameters() if p.requires_grad)\n",
    "print(f'\ud83d\udcca Parameters: {n_params_c:,}')\n",
    "\n",
    "# Train\n",
    "classical_history = train_model(\n",
    "    classical_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    CONFIG, \n",
    "    model_name='Classical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation (Simple Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple evaluation\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "def calculate_rmsd(coords_pred, coords_true):\n",
    "    \"\"\"Calculate RMSD after optimal alignment.\"\"\"\n",
    "    # Center\n",
    "    pred_centered = coords_pred - coords_pred.mean(axis=0)\n",
    "    true_centered = coords_true - coords_true.mean(axis=0)\n",
    "    \n",
    "    # Optimal rotation\n",
    "    R, _ = orthogonal_procrustes(pred_centered, true_centered)\n",
    "    pred_aligned = pred_centered @ R\n",
    "    \n",
    "    # RMSD\n",
    "    rmsd = np.sqrt(((pred_aligned - true_centered) ** 2).sum(axis=1).mean())\n",
    "    return rmsd\n",
    "\n",
    "def evaluate_simple(model, loader, name):\n",
    "    \"\"\"Simple evaluation function.\"\"\"\n",
    "    model.eval()\n",
    "    all_rmsd = []\n",
    "    all_plddt = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f'Evaluating {name}'):\n",
    "            inputs = batch['sequence']\n",
    "            targets = batch['coordinates']\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            coords_pred = outputs['coordinates'].cpu().numpy()\n",
    "            coords_true = targets.cpu().numpy()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            for i in range(len(coords_pred)):\n",
    "                rmsd = calculate_rmsd(coords_pred[i], coords_true[i])\n",
    "                all_rmsd.append(rmsd)\n",
    "            \n",
    "            if 'plddt' in outputs:\n",
    "                plddt = outputs['plddt'].cpu().numpy()\n",
    "                all_plddt.extend([p.mean() for p in plddt])\n",
    "    \n",
    "    return {\n",
    "        'rmsd': np.array(all_rmsd),\n",
    "        'plddt': np.array(all_plddt) if all_plddt else None\n",
    "    }\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('EVALUATION')\n",
    "print('='*80)\n",
    "\n",
    "quantum_results = evaluate_simple(quantum_model, test_loader, 'Quantum')\n",
    "classical_results = evaluate_simple(classical_model, test_loader, 'Classical')\n",
    "\n",
    "print('\\n\ud83d\udcca Results:')\n",
    "print('\\nQuantum:')\n",
    "print(f'  RMSD: {quantum_results[\"rmsd\"].mean():.4f} \u00b1 {quantum_results[\"rmsd\"].std():.4f} \u00c5')\n",
    "\n",
    "print('Classical:')\n",
    "print(f'  RMSD: {classical_results[\"rmsd\"].mean():.4f} \u00b1 {classical_results[\"rmsd\"].std():.4f} \u00c5')\n",
    "\n",
    "# Simple statistical test\n",
    "from scipy.stats import wilcoxon\n",
    "statistic, p_value = wilcoxon(quantum_results['rmsd'], classical_results['rmsd'])\n",
    "\n",
    "print(f'\\n\ud83d\udcca Wilcoxon test:')\n",
    "print(f'  p-value: {p_value:.4e}')\n",
    "\n",
    "if p_value < CONFIG['alpha']:\n",
    "    winner = 'Quantum' if quantum_results['rmsd'].mean() < classical_results['rmsd'].mean() else 'Classical'\n",
    "    print(f'  \u2705 {winner} is significantly better!')\n",
    "else:\n",
    "    print('  \u26a0\ufe0f  No significant difference detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Training curves\n",
    "ax = axes[0, 0]\n",
    "ax.plot(quantum_history['train_loss'], 'b-', label='Quantum Train', linewidth=2)\n",
    "ax.plot(quantum_history['val_loss'], 'b--', label='Quantum Val', linewidth=2)\n",
    "ax.plot(classical_history['train_loss'], 'r-', label='Classical Train', linewidth=2)\n",
    "ax.plot(classical_history['val_loss'], 'r--', label='Classical Val', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 2. RMSD distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(quantum_results['rmsd'], bins=15, alpha=0.6, label='Quantum', color='blue', edgecolor='black')\n",
    "ax.hist(classical_results['rmsd'], bins=15, alpha=0.6, label='Classical', color='orange', edgecolor='black')\n",
    "ax.set_xlabel('RMSD (\u00c5)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('RMSD Distribution', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 3. Training time\n",
    "ax = axes[1, 0]\n",
    "epochs = range(1, len(quantum_history['epoch_times']) + 1)\n",
    "ax.plot(epochs, np.cumsum(quantum_history['epoch_times']), 'b-', label='Quantum', linewidth=2, marker='o')\n",
    "ax.plot(epochs, np.cumsum(classical_history['epoch_times']), 'r-', label='Classical', linewidth=2, marker='s')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Time (s)', fontsize=12)\n",
    "ax.set_title('Training Time', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 4. Box plot comparison\n",
    "ax = axes[1, 1]\n",
    "data = [quantum_results['rmsd'], classical_results['rmsd']]\n",
    "bp = ax.boxplot(data, labels=['Quantum', 'Classical'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('blue')\n",
    "bp['boxes'][0].set_alpha(0.6)\n",
    "bp['boxes'][1].set_facecolor('orange')\n",
    "bp['boxes'][1].set_alpha(0.6)\n",
    "ax.set_ylabel('RMSD (\u00c5)', fontsize=12)\n",
    "ax.set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['output_dir']}/benchmark_summary.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\u2705 Visualization complete!')\n",
    "\n",
    "# Save to Drive if available\n",
    "if CONFIG['save_to_drive']:\n",
    "    !cp {CONFIG['output_dir']}/benchmark_summary.png {CONFIG['drive_path']}/\n",
    "    print(f'\ud83d\udcbe Saved to Google Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Compile results\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'quantum': {\n",
    "        'parameters': n_params_q,\n",
    "        'training_time': sum(quantum_history['epoch_times']),\n",
    "        'final_train_loss': quantum_history['train_loss'][-1],\n",
    "        'final_val_loss': quantum_history['val_loss'][-1],\n",
    "        'test_rmsd_mean': float(quantum_results['rmsd'].mean()),\n",
    "        'test_rmsd_std': float(quantum_results['rmsd'].std()),\n",
    "    },\n",
    "    'classical': {\n",
    "        'parameters': n_params_c,\n",
    "        'training_time': sum(classical_history['epoch_times']),\n",
    "        'final_train_loss': classical_history['train_loss'][-1],\n",
    "        'final_val_loss': classical_history['val_loss'][-1],\n",
    "        'test_rmsd_mean': float(classical_results['rmsd'].mean()),\n",
    "        'test_rmsd_std': float(classical_results['rmsd'].std()),\n",
    "    },\n",
    "    'statistical_test': {\n",
    "        'method': 'wilcoxon',\n",
    "        'p_value': float(p_value),\n",
    "        'alpha': CONFIG['alpha'],\n",
    "        'significant': p_value < CONFIG['alpha']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "results_path = f\"{CONFIG['output_dir']}/complete_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f'\u2705 Results saved to {results_path}')\n",
    "\n",
    "# Save to Drive\n",
    "if CONFIG['save_to_drive']:\n",
    "    !cp -r {CONFIG['output_dir']}/* {CONFIG['drive_path']}/\n",
    "    print(f'\ud83d\udcbe All results copied to Google Drive!')\n",
    "\n",
    "# Create archive for download\n",
    "!tar -czf benchmark_results.tar.gz -C {CONFIG['output_dir']} .\n",
    "print('\\n\ud83d\udce6 Created benchmark_results.tar.gz')\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print('Downloading results...')\n",
    "    try:\n",
    "        files.download('benchmark_results.tar.gz')\n",
    "        print('\u2705 Download started!')\n",
    "    except:\n",
    "        print('\ud83d\udcdd Download manually from Files panel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('BENCHMARK COMPLETE')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\n\ud83d\udcca Dataset: {CONFIG[\"train_samples\"]} train, {CONFIG[\"val_samples\"]} val, {CONFIG[\"test_samples\"]} test')\n",
    "print(f'\ud83d\udd27 Training: {CONFIG[\"epochs\"]} epochs')\n",
    "print(f'\u26a1 Device: {device}')\n",
    "\n",
    "print('\ud83d\udd35 Quantum Model:')\n",
    "print(f'  Parameters: {n_params_q:,}')\n",
    "print(f'  Final val loss: {quantum_history[\"val_loss\"][-1]:.4f}')\n",
    "print(f'  Test RMSD: {quantum_results[\"rmsd\"].mean():.4f} \u00b1 {quantum_results[\"rmsd\"].std():.4f} \u00c5')\n",
    "print(f'  Training time: {sum(quantum_history[\"epoch_times\"]):.1f}s')\n",
    "\n",
    "print('\ud83d\udd34 Classical Model:')\n",
    "print(f'  Parameters: {n_params_c:,}')\n",
    "print(f'  Final val loss: {classical_history[\"val_loss\"][-1]:.4f}')\n",
    "print(f'  Test RMSD: {classical_results[\"rmsd\"].mean():.4f} \u00b1 {classical_results[\"rmsd\"].std():.4f} \u00c5')\n",
    "print(f'  Training time: {sum(classical_history[\"epoch_times\"]):.1f}s')\n",
    "\n",
    "print('\ud83d\udcca Statistical Test:')\n",
    "print(f'  Wilcoxon p-value: {p_value:.4e}')\n",
    "print(f'  Significance level: {CONFIG[\"alpha\"]}')\n",
    "\n",
    "if p_value < CONFIG['alpha']:\n",
    "    if quantum_results['rmsd'].mean() < classical_results['rmsd'].mean():\n",
    "        print('  \u2705 \ud83c\udf89 Quantum model shows significant advantage!')\n",
    "    else:\n",
    "        print('  \u26a0\ufe0f  Classical model performs better')\n",
    "else:\n",
    "    print('  \ud83d\udd37 No significant difference')\n",
    "    print('     Tip: Increase epochs or samples for better statistical power')\n",
    "\n",
    "print('\\n\ud83d\udce6 Outputs:')\n",
    "print(f'  Local: {CONFIG[\"output_dir\"]}/') \n",
    "if CONFIG['save_to_drive']:\n",
    "    print(f'  Drive: {CONFIG[\"drive_path\"]}/') \n",
    "print(f'  Archive: benchmark_results.tar.gz')\n",
    "\n",
    "print('='*80)\n",
    "print('\u2705 ALL DONE!')\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}