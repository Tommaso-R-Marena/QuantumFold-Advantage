{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# QuantumFold-Advantage: Complete Production Benchmark\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/complete_production_run.ipynb)\n",
    "\n",
    "**Complete end-to-end training and benchmarking pipeline for publication-quality results**\n",
    "\n",
    "## üéØ What This Notebook Does\n",
    "\n",
    "This notebook runs the **complete research pipeline** to generate publication-ready results:\n",
    "\n",
    "1. ‚úÖ **Data Preparation** - Download and process CATH protein structures\n",
    "2. ‚úÖ **Quantum Model Training** - Full training with ESM-2 embeddings + quantum layers\n",
    "3. ‚úÖ **Classical Baseline Training** - Identical architecture without quantum enhancement\n",
    "4. ‚úÖ **Comprehensive Evaluation** - TM-score, RMSD, GDT-TS, pLDDT on test set\n",
    "5. ‚úÖ **Statistical Validation** - Hypothesis tests, effect sizes, confidence intervals\n",
    "6. ‚úÖ **Publication Figures** - Training curves, distributions, comparison plots\n",
    "7. ‚úÖ **Results Export** - Trained models, metrics, plots saved to Google Drive\n",
    "\n",
    "## ‚öôÔ∏è Requirements\n",
    "\n",
    "**Recommended Setup:**\n",
    "- **Colab Pro/Pro+** with A100 GPU (40GB VRAM)\n",
    "- **High RAM** runtime\n",
    "- **~4-6 hours** total runtime\n",
    "\n",
    "**Free Tier Compatibility:**\n",
    "- ‚ö†Ô∏è Possible but will require reducing dataset size and model parameters\n",
    "- See optimization flags in configuration cell\n",
    "\n",
    "## üìä Expected Results\n",
    "\n",
    "After completion, you'll have:\n",
    "- **Trained quantum model** (~200MB checkpoint)\n",
    "- **Trained classical baseline** (~200MB checkpoint)\n",
    "- **Performance metrics** (JSON + CSV)\n",
    "- **Statistical analysis** (p-values, effect sizes, CI)\n",
    "- **Publication figures** (10+ high-resolution plots)\n",
    "- **Complete results archive** (ZIP for download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "setup_check"
   },
   "outputs": [],
   "source": [
    "#@title üîç Environment Check\n",
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print('=' * 80)\n",
    "print('ENVIRONMENT CHECK')\n",
    "print('=' * 80)\n",
    "\n",
    "# GPU Check\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'‚úÖ GPU: {gpu_name}')\n",
    "    print(f'‚úÖ VRAM: {gpu_memory:.1f}GB')\n",
    "    \n",
    "    if 'A100' in gpu_name:\n",
    "        print('üî• OPTIMAL: A100 detected - full pipeline enabled')\n",
    "    elif 'V100' in gpu_name:\n",
    "        print('‚úÖ GOOD: V100 detected - full pipeline possible')\n",
    "    elif 'T4' in gpu_name:\n",
    "        print('‚ö†Ô∏è  WARNING: T4 detected - consider reducing model size')\n",
    "        print('   Set USE_REDUCED_CONFIG=True below')\n",
    "    else:\n",
    "        print(f'‚ö†Ô∏è  WARNING: {gpu_name} - may need optimization')\n",
    "else:\n",
    "    print('‚ùå ERROR: No GPU detected!')\n",
    "    print('   Runtime ‚Üí Change runtime type ‚Üí GPU')\n",
    "    sys.exit(1)\n",
    "\n",
    "# RAM Check\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print(f'\\nüíæ System RAM: {ram_gb:.1f}GB')\n",
    "if ram_gb >= 50:\n",
    "    print('‚úÖ High RAM runtime detected - optimal for large datasets')\n",
    "elif ram_gb >= 25:\n",
    "    print('‚úÖ Standard RAM - sufficient for full pipeline')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Low RAM - consider enabling High RAM runtime')\n",
    "\n",
    "# Disk Check\n",
    "disk = psutil.disk_usage('/')\n",
    "disk_free = disk.free / 1e9\n",
    "print(f'\\nüíø Free Disk: {disk_free:.1f}GB')\n",
    "if disk_free < 10:\n",
    "    print('‚ö†Ô∏è  WARNING: Low disk space (<10GB)')\n",
    "\n",
    "# Colab Check\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print('\\n‚úÖ Google Colab environment detected')\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    print('\\n‚ö†Ô∏è  Not running in Colab - some features may be limited')\n",
    "    IS_COLAB = False\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('Ready to proceed!')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "#@title üìÅ Mount Google Drive (Recommended for saving results)\n",
    "\n",
    "MOUNT_DRIVE = True  #@param {type:\"boolean\"}\n",
    "\n",
    "if MOUNT_DRIVE and IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create results directory\n",
    "    RESULTS_DIR = '/content/drive/MyDrive/QuantumFold_Results'\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    print(f'‚úÖ Results will be saved to: {RESULTS_DIR}')\n",
    "else:\n",
    "    RESULTS_DIR = '/content/results'\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    print(f'‚ö†Ô∏è  Results will be saved locally to: {RESULTS_DIR}')\n",
    "    print('   (Download manually before session ends)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install QuantumFold-Advantage and dependencies\n",
    "get_ipython().system('pip install -q git+https://github.com/Tommaso-R-Marena/QuantumFold-Advantage.git')\n",
    "get_ipython().system('pip install -q fair-esm biopython pennylane pennylane-qiskit')\n",
    "get_ipython().system('pip install -q wandb tensorboard scipy scikit-learn matplotlib seaborn plotly')\n",
    "get_ipython().system('pip install -q einops py3Dmol MDAnalysis')\n",
    "\n",
    "print('‚úÖ All dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# QuantumFold imports\n",
    "from src.advanced_model import AdvancedProteinFoldingModel\n",
    "from src.protein_embeddings import ESM2Embedder\n",
    "from src.data import ProteinDataset, fetch_pdb_structures as download_pdb_structures\n",
    "from src.advanced_training import AdvancedTrainer\n",
    "from src.benchmarks import compute_tm_score, compute_rmsd, compute_gdt_ts\n",
    "from src.statistical_validation import ComprehensiveBenchmark\n",
    "from src.reproducibility import set_seed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üî• Using device: {device}')\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print('‚úÖ Imports complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "configuration"
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "#@title ‚öôÔ∏è Configuration\n",
    "\n",
    "#@markdown ### Hardware Optimization\n",
    "USE_REDUCED_CONFIG = False  #@param {type:\"boolean\"}\n",
    "#@markdown Enable for T4 GPU or Free Tier Colab\n",
    "\n",
    "#@markdown ### Training Configuration\n",
    "NUM_TRAINING_PROTEINS = 100  #@param {type:\"slider\", min:50, max:500, step:50}\n",
    "NUM_EPOCHS_QUANTUM = 50  #@param {type:\"slider\", min:10, max:100, step:10}\n",
    "NUM_EPOCHS_CLASSICAL = 50  #@param {type:\"slider\", min:10, max:100, step:10}\n",
    "BATCH_SIZE = 4  #@param {type:\"slider\", min:1, max:16, step:1}\n",
    "\n",
    "#@markdown ### Model Configuration\n",
    "ESM_MODEL = \"esm2_t33_650M_UR50D\"  #@param [\"esm2_t33_650M_UR50D\", \"esm2_t36_3B_UR50D\"]\n",
    "HIDDEN_DIM = 384  #@param {type:\"slider\", min:128, max:768, step:128}\n",
    "NUM_STRUCTURE_LAYERS = 4  #@param {type:\"slider\", min:2, max:8, step:1}\n",
    "\n",
    "#@markdown ### Quantum Configuration\n",
    "NUM_QUBITS = 8  #@param {type:\"slider\", min:4, max:16, step:2}\n",
    "NUM_QUANTUM_LAYERS = 3  #@param {type:\"slider\", min:1, max:5, step:1}\n",
    "NOISE_LEVEL = 0.01  #@param {type:\"slider\", min:0.0, max:0.1, step:0.01}\n",
    "\n",
    "#@markdown ### Advanced Options\n",
    "USE_MIXED_PRECISION = True  #@param {type:\"boolean\"}\n",
    "USE_EMA = True  #@param {type:\"boolean\"}\n",
    "USE_GRADIENT_CHECKPOINTING = True  #@param {type:\"boolean\"}\n",
    "WANDB_LOGGING = False  #@param {type:\"boolean\"}\n",
    "\n",
    "# Apply reduced config if needed\n",
    "if USE_REDUCED_CONFIG:\n",
    "    print('‚öôÔ∏è  Applying reduced configuration for T4/Free Tier...')\n",
    "    NUM_TRAINING_PROTEINS = min(NUM_TRAINING_PROTEINS, 50)\n",
    "    ESM_MODEL = \"esm2_t33_650M_UR50D\"\n",
    "    HIDDEN_DIM = min(HIDDEN_DIM, 256)\n",
    "    NUM_STRUCTURE_LAYERS = min(NUM_STRUCTURE_LAYERS, 3)\n",
    "    NUM_QUBITS = min(NUM_QUBITS, 6)\n",
    "    BATCH_SIZE = min(BATCH_SIZE, 2)\n",
    "\n",
    "# Build config dictionary\n",
    "CONFIG = {\n",
    "    'hardware': {\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "        'vram_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,\n",
    "        'ram_gb': psutil.virtual_memory().total / 1e9,\n",
    "        'reduced_config': USE_REDUCED_CONFIG\n",
    "    },\n",
    "    'data': {\n",
    "        'num_proteins': NUM_TRAINING_PROTEINS,\n",
    "        'batch_size': BATCH_SIZE\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs_quantum': NUM_EPOCHS_QUANTUM,\n",
    "        'epochs_classical': NUM_EPOCHS_CLASSICAL,\n",
    "        'mixed_precision': USE_MIXED_PRECISION,\n",
    "        'ema': USE_EMA,\n",
    "        'gradient_checkpointing': USE_GRADIENT_CHECKPOINTING\n",
    "    },\n",
    "    'model': {\n",
    "        'esm_model': ESM_MODEL,\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'num_structure_layers': NUM_STRUCTURE_LAYERS\n",
    "    },\n",
    "    'quantum': {\n",
    "        'num_qubits': NUM_QUBITS,\n",
    "        'num_layers': NUM_QUANTUM_LAYERS,\n",
    "        'noise_level': NOISE_LEVEL\n",
    "    },\n",
    "    'experiment': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'seed': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save config\n",
    "config_path = os.path.join(RESULTS_DIR, 'experiment_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('EXPERIMENT CONFIGURATION')\n",
    "print('=' * 80)\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "print('=' * 80)\n",
    "print(f'\\n‚úÖ Configuration saved to: {config_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep_header"
   },
   "source": [
    "## üìä Step 1: Data Preparation\n",
    "\n",
    "Download protein structures from PDB and prepare datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_generation"
   },
   "outputs": [],
   "source": [
    "def generate_diverse_protein_dataset(n_proteins=100):\n",
    "    \"\"\"Generate diverse set of PDB IDs from different structure classes\"\"\"\n",
    "    \n",
    "    # Alpha helical proteins (30%)\n",
    "    alpha_proteins = [\n",
    "        '1MBN', '1MYO', '1MYG', '256B', '1LFB', '1HMK', '1HCL', '1A6N', '1BVC', '1COA',\n",
    "        '1CRL', '1D3B', '1DLW', '1ECD', '1FLP', '1G6N', '1H6W', '1IA0', '1JBO', '1K40',\n",
    "        '1LFD', '1M6T', '1N0J', '1O06', '1PMY', '1QLA', '1R69', '1S72', '1TRZ', '1UHA'\n",
    "    ]\n",
    "    \n",
    "    # Beta sheet proteins (30%)\n",
    "    beta_proteins = [\n",
    "        '1TEN', '1FNA', '1BNL', '1EAL', '1FMM', '1G2R', '1H0H', '1I2T', '1JB0', '1K20',\n",
    "        '1L5B', '1M3S', '1N0U', '1O5R', '1P9I', '1QDD', '1R7J', '1S6V', '1T2F', '1U2H',\n",
    "        '1BRS', '1BTH', '1CDG', '1CEW', '1CLV', '1DFJ', '1EJG', '1ETM', '1FCH', '1FIE'\n",
    "    ]\n",
    "    \n",
    "    # Mixed alpha/beta (30%)\n",
    "    mixed_proteins = [\n",
    "        '1UBQ', '1CRN', '2MLT', '1PGB', '5CRO', '4PTI', '1SHG', '2CI2', '1BPI', '1YCC',\n",
    "        '1AKI', '1BBA', '3CHY', '1BP2', '1LMB', '2LZM', '1CSE', '1HRC', '1CTF', '1SBP',\n",
    "        '1A0P', '1A2P', '1A3A', '1A49', '1A53', '1A62', '1AIE', '1AK9', '1AKZ', '1ALY'\n",
    "    ]\n",
    "    \n",
    "    # Small proteins for validation (10%)\n",
    "    small_proteins = [\n",
    "        '1VII', '2K39', '1ENH', '1RIS', '5TRV', '1L2Y', '2MJB', '1MB6', '2ERL', '1IGD'\n",
    "    ]\n",
    "    \n",
    "    # Combine and sample\n",
    "    all_proteins = alpha_proteins + beta_proteins + mixed_proteins + small_proteins\n",
    "    \n",
    "    # Ensure we don't exceed available\n",
    "    n_proteins = min(n_proteins, len(all_proteins))\n",
    "    \n",
    "    # Sample with balanced representation\n",
    "    np.random.seed(42)\n",
    "    selected = []\n",
    "    \n",
    "    n_alpha = int(0.3 * n_proteins)\n",
    "    n_beta = int(0.3 * n_proteins)\n",
    "    n_mixed = int(0.3 * n_proteins)\n",
    "    n_small = n_proteins - n_alpha - n_beta - n_mixed\n",
    "    \n",
    "    selected.extend(np.random.choice(alpha_proteins, min(n_alpha, len(alpha_proteins)), replace=False))\n",
    "    selected.extend(np.random.choice(beta_proteins, min(n_beta, len(beta_proteins)), replace=False))\n",
    "    selected.extend(np.random.choice(mixed_proteins, min(n_mixed, len(mixed_proteins)), replace=False))\n",
    "    selected.extend(np.random.choice(small_proteins, min(n_small, len(small_proteins)), replace=False))\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# Generate dataset\n",
    "print(f'üìä Generating dataset of {NUM_TRAINING_PROTEINS} proteins...')\n",
    "pdb_ids = generate_diverse_protein_dataset(NUM_TRAINING_PROTEINS)\n",
    "print(f'‚úÖ Selected {len(pdb_ids)} proteins')\n",
    "print(f'   Classes: ~30% alpha, ~30% beta, ~30% mixed, ~10% small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_structures"
   },
   "outputs": [],
   "source": [
    "print('\\nüì• Downloading PDB structures...')\n",
    "print(f'   This may take 5-10 minutes for {len(pdb_ids)} proteins\\n')\n",
    "\n",
    "structures = download_pdb_structures(pdb_ids, max_workers=10)\n",
    "\n",
    "print(f'\\n‚úÖ Successfully downloaded {len(structures)} structures')\n",
    "print(f'‚ùå Failed: {len(pdb_ids) - len(structures)} proteins')\n",
    "\n",
    "# Print statistics\n",
    "lengths = [len(s['coords']) for s in structures.values()]\n",
    "print(f'\\nüìà Dataset Statistics:')\n",
    "print(f'   Mean length: {np.mean(lengths):.1f} residues')\n",
    "print(f'   Min length: {np.min(lengths)} residues')\n",
    "print(f'   Max length: {np.max(lengths)} residues')\n",
    "print(f'   Median length: {np.median(lengths):.1f} residues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_test_split"
   },
   "outputs": [],
   "source": [
    "# Split data: 70% train, 15% val, 15% test\n",
    "pdb_list = list(structures.keys())\n",
    "np.random.shuffle(pdb_list)\n",
    "\n",
    "n_total = len(pdb_list)\n",
    "n_train = int(0.70 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "train_ids = pdb_list[:n_train]\n",
    "val_ids = pdb_list[n_train:n_train+n_val]\n",
    "test_ids = pdb_list[n_train+n_val:]\n",
    "\n",
    "print(f'\\nüìä Data Split:')\n",
    "print(f'   Train: {len(train_ids)} proteins')\n",
    "print(f'   Val:   {len(val_ids)} proteins')\n",
    "print(f'   Test:  {len(test_ids)} proteins')\n",
    "\n",
    "# Save split\n",
    "split_info = {\n",
    "    'train': train_ids,\n",
    "    'val': val_ids,\n",
    "    'test': test_ids\n",
    "}\n",
    "\n",
    "split_path = os.path.join(RESULTS_DIR, 'data_split.json')\n",
    "with open(split_path, 'w') as f:\n",
    "    json.dump(split_info, f, indent=2)\n",
    "\n",
    "print(f'‚úÖ Split saved to: {split_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "embeddings_header"
   },
   "source": [
    "## üß¨ Step 2: Generate ESM-2 Embeddings\n",
    "\n",
    "Generate pre-trained protein language model embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_embeddings"
   },
   "outputs": [],
   "source": [
    "print(f'üß† Loading ESM-2 model: {ESM_MODEL}...')\n",
    "embedder = ESM2Embedder(model_name=ESM_MODEL, device=device)\n",
    "print(f'‚úÖ Model loaded (embedding dim: {embedder.embedding_dim})')\n",
    "\n",
    "print(f'\\nüîÑ Generating embeddings for {len(structures)} proteins...')\n",
    "print('   This may take 10-20 minutes depending on GPU\\n')\n",
    "\n",
    "# Generate embeddings in batches to save memory\n",
    "EMBEDDING_BATCH_SIZE = 5\n",
    "embedding_cache_dir = os.path.join(RESULTS_DIR, 'embedding_cache')\n",
    "os.makedirs(embedding_cache_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(0, len(pdb_list), EMBEDDING_BATCH_SIZE), desc='Generating embeddings'):\n",
    "    batch_ids = pdb_list[i:i+EMBEDDING_BATCH_SIZE]\n",
    "    batch_seqs = [structures[pdb_id]['sequence'] for pdb_id in batch_ids]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    batch_embeddings = embedder(batch_seqs)['embeddings']\n",
    "    \n",
    "    # Save to cache\n",
    "    for pdb_id, emb in zip(batch_ids, batch_embeddings):\n",
    "        cache_path = os.path.join(embedding_cache_dir, f'{pdb_id}.pt')\n",
    "        torch.save(emb.cpu(), cache_path)\n",
    "        structures[pdb_id]['embedding_path'] = cache_path\n",
    "    \n",
    "    # Clear cache\n",
    "    del batch_embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Free ESM model\n",
    "del embedder\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f'\\n‚úÖ Embeddings cached to: {embedding_cache_dir}')\n",
    "print(f'üßπ ESM model freed from memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = ProteinDataset(train_ids, structures, augment=True)\n",
    "val_dataset = ProteinDataset(val_ids, structures, augment=False)\n",
    "test_dataset = ProteinDataset(test_ids, structures, augment=False)\n",
    "\n",
    "# Create dataloaders\n",
    "from src.data_processing import collate_fn\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'‚úÖ DataLoaders created:')\n",
    "print(f'   Train batches: {len(train_loader)}')\n",
    "print(f'   Val batches:   {len(val_loader)}')\n",
    "print(f'   Test batches:  {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quantum_training_header"
   },
   "source": [
    "## ‚öõÔ∏è Step 3: Train Quantum-Enhanced Model\n",
    "\n",
    "Train the full model with quantum layers enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_quantum_model"
   },
   "outputs": [],
   "source": [
    "print('‚öõÔ∏è  Initializing Quantum-Enhanced Model...')\n",
    "\n",
    "# Determine input dimension from ESM model\n",
    "if '650M' in ESM_MODEL:\n",
    "    esm_dim = 1280\n",
    "elif '3B' in ESM_MODEL:\n",
    "    esm_dim = 2560\n",
    "else:\n",
    "    esm_dim = 1280  # default\n",
    "\n",
    "quantum_model = AdvancedProteinFoldingModel(\n",
    "    input_dim=esm_dim,\n",
    "    c_s=HIDDEN_DIM,\n",
    "    c_z=HIDDEN_DIM // 3,\n",
    "    num_structure_layers=NUM_STRUCTURE_LAYERS,\n",
    "    use_quantum=True,\n",
    "    num_qubits=NUM_QUBITS,\n",
    "    num_quantum_layers=NUM_QUANTUM_LAYERS,\n",
    "    noise_level=NOISE_LEVEL\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in quantum_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in quantum_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'‚úÖ Model initialized:')\n",
    "print(f'   Total parameters: {total_params:,}')\n",
    "print(f'   Trainable parameters: {trainable_params:,}')\n",
    "print(f'   Model size: ~{total_params * 4 / 1e6:.1f}MB')\n",
    "print(f'   Quantum: {NUM_QUBITS} qubits, {NUM_QUANTUM_LAYERS} layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_quantum_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "quantum_trainer = AdvancedTrainer(\n",
    "    model=quantum_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    learning_rate=5e-4,\n",
    "    use_amp=USE_MIXED_PRECISION,\n",
    "    use_ema=USE_EMA,\n",
    "    gradient_clip=1.0,\n",
    "    output_dir=os.path.join(RESULTS_DIR, 'quantum_model'),\n",
    "    use_wandb=WANDB_LOGGING\n",
    ")\n",
    "\n",
    "print('‚úÖ Quantum trainer initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_quantum_model"
   },
   "outputs": [],
   "source": [
    "print(f'\\nüöÄ Training Quantum-Enhanced Model for {NUM_EPOCHS_QUANTUM} epochs...')\n",
    "print(f'   Estimated time: ~{NUM_EPOCHS_QUANTUM * len(train_loader) * 2 / 60:.0f} minutes\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "quantum_history = quantum_trainer.train(\n",
    "    num_epochs=NUM_EPOCHS_QUANTUM,\n",
    "    save_freq=10,\n",
    "    val_freq=5\n",
    ")\n",
    "\n",
    "quantum_training_time = time.time() - start_time\n",
    "\n",
    "print(f'\\n‚úÖ Quantum model training complete!')\n",
    "print(f'   Total time: {quantum_training_time/60:.1f} minutes')\n",
    "print(f'   Best val loss: {min(quantum_history[\"val_loss\"]):.4f}')\n",
    "\n",
    "# Save training history\n",
    "history_path = os.path.join(RESULTS_DIR, 'quantum_model', 'training_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(quantum_history, f, indent=2)\n",
    "\n",
    "print(f'   History saved to: {history_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "classical_training_header"
   },
   "source": [
    "## üî¨ Step 4: Train Classical Baseline\n",
    "\n",
    "Train identical model with quantum layers disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_classical_model"
   },
   "outputs": [],
   "source": [
    "print('üî¨ Initializing Classical Baseline Model...')\n",
    "\n",
    "# Free quantum model from GPU to save memory\n",
    "quantum_model = quantum_model.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "classical_model = AdvancedProteinFoldingModel(\n",
    "    input_dim=esm_dim,\n",
    "    c_s=HIDDEN_DIM,\n",
    "    c_z=HIDDEN_DIM // 3,\n",
    "    num_structure_layers=NUM_STRUCTURE_LAYERS,\n",
    "    use_quantum=False  # DISABLED\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in classical_model.parameters())\n",
    "\n",
    "print(f'‚úÖ Classical model initialized:')\n",
    "print(f'   Total parameters: {total_params:,}')\n",
    "print(f'   Model size: ~{total_params * 4 / 1e6:.1f}MB')\n",
    "print(f'   Quantum: DISABLED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_classical_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "classical_trainer = AdvancedTrainer(\n",
    "    model=classical_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    learning_rate=5e-4,\n",
    "    use_amp=USE_MIXED_PRECISION,\n",
    "    use_ema=USE_EMA,\n",
    "    gradient_clip=1.0,\n",
    "    output_dir=os.path.join(RESULTS_DIR, 'classical_model'),\n",
    "    use_wandb=WANDB_LOGGING\n",
    ")\n",
    "\n",
    "print('‚úÖ Classical trainer initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_classical_model"
   },
   "outputs": [],
   "source": [
    "print(f'\\nüöÄ Training Classical Baseline for {NUM_EPOCHS_CLASSICAL} epochs...')\n",
    "print(f'   Estimated time: ~{NUM_EPOCHS_CLASSICAL * len(train_loader) * 2 / 60:.0f} minutes\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "classical_history = classical_trainer.train(\n",
    "    num_epochs=NUM_EPOCHS_CLASSICAL,\n",
    "    save_freq=10,\n",
    "    val_freq=5\n",
    ")\n",
    "\n",
    "classical_training_time = time.time() - start_time\n",
    "\n",
    "print(f'\\n‚úÖ Classical model training complete!')\n",
    "print(f'   Total time: {classical_training_time/60:.1f} minutes')\n",
    "print(f'   Best val loss: {min(classical_history[\"val_loss\"]):.4f}')\n",
    "\n",
    "# Save training history\n",
    "history_path = os.path.join(RESULTS_DIR, 'classical_model', 'training_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(classical_history, f, indent=2)\n",
    "\n",
    "print(f'   History saved to: {history_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_header"
   },
   "source": [
    "## üìà Step 5: Comprehensive Evaluation\n",
    "\n",
    "Evaluate both models on test set with all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_models"
   },
   "outputs": [],
   "source": [
    "print('üìä Evaluating models on test set...\\n')\n",
    "\n",
    "# Load best checkpoints\n",
    "quantum_model.load_state_dict(torch.load(\n",
    "    os.path.join(RESULTS_DIR, 'quantum_model', 'best_model.pt'),\n",
    "    map_location=device\n",
    "))\n",
    "quantum_model.eval()\n",
    "\n",
    "classical_model.load_state_dict(torch.load(\n",
    "    os.path.join(RESULTS_DIR, 'classical_model', 'best_model.pt'),\n",
    "    map_location=device\n",
    "))\n",
    "classical_model.eval()\n",
    "\n",
    "print('‚úÖ Best checkpoints loaded')\n",
    "\n",
    "def evaluate_model(model, dataloader, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    results = {\n",
    "        'tm_scores': [],\n",
    "        'rmsds': [],\n",
    "        'gdt_ts': [],\n",
    "        'plddts': []\n",
    "    }\n",
    "    \n",
    "    print(f'\\nEvaluating {model_name}...')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f'{model_name} evaluation'):\n",
    "            embeddings = batch['embedding'].to(device)\n",
    "            true_coords = batch['coords'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(embeddings, mask=mask)\n",
    "            pred_coords = output['coordinates']\n",
    "            plddt = output.get('plddt', None)\n",
    "            \n",
    "            # Compute metrics for each example in batch\n",
    "            for i in range(pred_coords.shape[0]):\n",
    "                m = mask[i].cpu().bool()\n",
    "                pred = pred_coords[i][m].cpu().numpy()\n",
    "                true = true_coords[i][m].cpu().numpy()\n",
    "                \n",
    "                if len(pred) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # TM-score\n",
    "                tm = compute_tm_score(pred, true)\n",
    "                results['tm_scores'].append(tm)\n",
    "                \n",
    "                # RMSD\n",
    "                rmsd = compute_rmsd(pred, true)\n",
    "                results['rmsds'].append(rmsd)\n",
    "                \n",
    "                # GDT-TS\n",
    "                gdt = compute_gdt_ts(pred, true)\n",
    "                results['gdt_ts'].append(gdt)\n",
    "                \n",
    "                # pLDDT\n",
    "                if plddt is not None:\n",
    "                    results['plddts'].append(plddt[i][m].mean().item())\n",
    "    \n",
    "    # Convert to arrays\n",
    "    for key in results:\n",
    "        results[key] = np.array(results[key])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate both models\n",
    "quantum_results = evaluate_model(quantum_model, test_loader, 'Quantum')\n",
    "classical_results = evaluate_model(classical_model, test_loader, 'Classical')\n",
    "\n",
    "print('\\n‚úÖ Evaluation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "print_results"
   },
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "print('\\n' + '=' * 80)\n",
    "print('TEST SET RESULTS')\n",
    "print('=' * 80)\n",
    "\n",
    "metrics = ['tm_scores', 'rmsds', 'gdt_ts', 'plddts']\n",
    "metric_names = ['TM-score', 'RMSD (√Ö)', 'GDT-TS', 'pLDDT']\n",
    "\n",
    "for metric, name in zip(metrics, metric_names):\n",
    "    if len(quantum_results[metric]) == 0:\n",
    "        continue\n",
    "    \n",
    "    q_mean = np.mean(quantum_results[metric])\n",
    "    q_std = np.std(quantum_results[metric])\n",
    "    c_mean = np.mean(classical_results[metric])\n",
    "    c_std = np.std(classical_results[metric])\n",
    "    \n",
    "    print(f'\\n{name}:')\n",
    "    print(f'  Quantum:   {q_mean:.4f} ¬± {q_std:.4f}')\n",
    "    print(f'  Classical: {c_mean:.4f} ¬± {c_std:.4f}')\n",
    "    \n",
    "    diff = q_mean - c_mean\n",
    "    if metric == 'rmsds':\n",
    "        better = diff < 0\n",
    "    else:\n",
    "        better = diff > 0\n",
    "    \n",
    "    symbol = '‚úÖ' if better else '‚ùå'\n",
    "    print(f'  Difference: {diff:+.4f} {symbol}')\n",
    "\n",
    "print('\\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stats_header"
   },
   "source": [
    "## üìä Step 6: Statistical Validation\n",
    "\n",
    "Rigorous statistical testing for quantum advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "statistical_tests"
   },
   "outputs": [],
   "source": [
    "print('üìä Running statistical validation...\\n')\n",
    "\n",
    "benchmark = ComprehensiveBenchmark(\n",
    "    output_dir=os.path.join(RESULTS_DIR, 'statistical_analysis')\n",
    ")\n",
    "\n",
    "# Test for each metric\n",
    "stat_results = {}\n",
    "\n",
    "for metric, name in zip(metrics, metric_names):\n",
    "    if len(quantum_results[metric]) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f'\\nTesting {name}...')\n",
    "    \n",
    "    higher_is_better = metric != 'rmsds'\n",
    "    \n",
    "    result = benchmark.compare_methods(\n",
    "        quantum_scores=quantum_results[metric],\n",
    "        classical_scores=classical_results[metric],\n",
    "        metric_name=name,\n",
    "        higher_is_better=higher_is_better\n",
    "    )\n",
    "    \n",
    "    stat_results[name] = result\n",
    "    \n",
    "    print(f'  Wilcoxon p-value: {result[\"wilcoxon_p\"]:}')\n",
    "    print(f'  t-test p-value: {result[\"ttest_p\"]:.4f}')\n",
    "    print(f\"  Cohen's d: {result['cohens_d']:.4f}\")\n",
    "    print(f'  95% CI: [{result[\"ci_lower\"]:.4f}, {result[\"ci_upper\"]:.4f}]')\n",
    "    \n",
    "    if result['wilcoxon_p'] < 0.05:\n",
    "        print(f'  ‚úÖ SIGNIFICANT at p<0.05')\n",
    "    else:\n",
    "        print(f'  ‚ùå Not significant at p<0.05')\n",
    "\n",
    "# Save statistical results\n",
    "stats_path = os.path.join(RESULTS_DIR, 'statistical_analysis', 'results.json')\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(stat_results, f, indent=2)\n",
    "\n",
    "print(f'\\n‚úÖ Statistical analysis saved to: {stats_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viz_header"
   },
   "source": [
    "## üìà Step 7: Generate Publication Figures\n",
    "\n",
    "Create high-quality plots for publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_training_curves"
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Train loss\n",
    "axes[0, 0].plot(quantum_history['train_loss'], label='Quantum', linewidth=2)\n",
    "axes[0, 0].plot(classical_history['train_loss'], label='Classical', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Training Loss')\n",
    "axes[0, 0].set_title('Training Loss Curves')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Val loss\n",
    "axes[0, 1].plot(quantum_history['val_loss'], label='Quantum', linewidth=2)\n",
    "axes[0, 1].plot(classical_history['val_loss'], label='Classical', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Validation Loss')\n",
    "axes[0, 1].set_title('Validation Loss Curves')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 0].plot(quantum_history.get('learning_rate', []), linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "times = [quantum_training_time/60, classical_training_time/60]\n",
    "axes[1, 1].bar(['Quantum', 'Classical'], times, color=['#2E86AB', '#A23B72'])\n",
    "axes[1, 1].set_ylabel('Training Time (minutes)')\n",
    "axes[1, 1].set_title('Training Time Comparison')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'training_curves.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Training curves saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_distributions"
   },
   "outputs": [],
   "source": [
    "# Plot metric distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics[:4], metric_names[:4])):\n",
    "    if len(quantum_results[metric]) == 0:\n",
    "        continue\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Violin plots\n",
    "    data = [quantum_results[metric], classical_results[metric]]\n",
    "    parts = ax.violinplot(data, positions=[1, 2], showmeans=True, showextrema=True)\n",
    "    \n",
    "    # Color the violins\n",
    "    for pc, color in zip(parts['bodies'], ['#2E86AB', '#A23B72']):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.6)\n",
    "    \n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_xticklabels(['Quantum', 'Classical'])\n",
    "    ax.set_ylabel(name)\n",
    "    ax.set_title(f'{name} Distribution')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'metric_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Distribution plots saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_comparison"
   },
   "outputs": [],
   "source": [
    "# Paired comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics[:4], metric_names[:4])):\n",
    "    if len(quantum_results[metric]) == 0:\n",
    "        continue\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(classical_results[metric], quantum_results[metric], \n",
    "               alpha=0.6, s=100, edgecolors='black', linewidths=0.5)\n",
    "    \n",
    "    # Diagonal line (y=x)\n",
    "    lims = [\n",
    "        np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "        np.max([ax.get_xlim(), ax.get_ylim()])\n",
    "    ]\n",
    "    ax.plot(lims, lims, 'k--', alpha=0.5, zorder=0, label='Equal Performance')\n",
    "    \n",
    "    ax.set_xlabel(f'Classical {name}')\n",
    "    ax.set_ylabel(f'Quantum {name}')\n",
    "    ax.set_title(f'Paired Comparison: {name}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'paired_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Comparison plots saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_header"
   },
   "source": [
    "## üíæ Step 8: Export Results\n",
    "\n",
    "Save all results for download and future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_raw_results"
   },
   "outputs": [],
   "source": [
    "# Save raw results as CSV\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'quantum_tm_score': quantum_results['tm_scores'],\n",
    "    'classical_tm_score': classical_results['tm_scores'],\n",
    "    'quantum_rmsd': quantum_results['rmsds'],\n",
    "    'classical_rmsd': classical_results['rmsds'],\n",
    "    'quantum_gdt_ts': quantum_results['gdt_ts'],\n",
    "    'classical_gdt_ts': classical_results['gdt_ts']\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(RESULTS_DIR, 'raw_results.csv')\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f'‚úÖ Raw results saved to: {csv_path}')\n",
    "\n",
    "# Display first few rows\n",
    "print('\\nFirst 5 results:')\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_summary_report"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive summary report\n",
    "summary = {\n",
    "    'experiment': {\n",
    "        'timestamp': CONFIG['experiment']['timestamp'],\n",
    "        'seed': CONFIG['experiment']['seed'],\n",
    "        'total_runtime_minutes': (quantum_training_time + classical_training_time) / 60\n",
    "    },\n",
    "    'hardware': CONFIG['hardware'],\n",
    "    'configuration': {\n",
    "        'num_proteins': NUM_TRAINING_PROTEINS,\n",
    "        'train_proteins': len(train_ids),\n",
    "        'val_proteins': len(val_ids),\n",
    "        'test_proteins': len(test_ids),\n",
    "        'epochs_quantum': NUM_EPOCHS_QUANTUM,\n",
    "        'epochs_classical': NUM_EPOCHS_CLASSICAL,\n",
    "        'batch_size': BATCH_SIZE\n",
    "    },\n",
    "    'quantum_results': {\n",
    "        'tm_score': {\n",
    "            'mean': float(np.mean(quantum_results['tm_scores'])),\n",
    "            'std': float(np.std(quantum_results['tm_scores'])),\n",
    "            'median': float(np.median(quantum_results['tm_scores']))\n",
    "        },\n",
    "        'rmsd': {\n",
    "            'mean': float(np.mean(quantum_results['rmsds'])),\n",
    "            'std': float(np.std(quantum_results['rmsds'])),\n",
    "            'median': float(np.median(quantum_results['rmsds']))\n",
    "        },\n",
    "        'gdt_ts': {\n",
    "            'mean': float(np.mean(quantum_results['gdt_ts'])),\n",
    "            'std': float(np.std(quantum_results['gdt_ts'])),\n",
    "            'median': float(np.median(quantum_results['gdt_ts']))\n",
    "        }\n",
    "    },\n",
    "    'classical_results': {\n",
    "        'tm_score': {\n",
    "            'mean': float(np.mean(classical_results['tm_scores'])),\n",
    "            'std': float(np.std(classical_results['tm_scores'])),\n",
    "            'median': float(np.median(classical_results['tm_scores']))\n",
    "        },\n",
    "        'rmsd': {\n",
    "            'mean': float(np.mean(classical_results['rmsds'])),\n",
    "            'std': float(np.std(classical_results['rmsds'])),\n",
    "            'median': float(np.median(classical_results['rmsds']))\n",
    "        },\n",
    "        'gdt_ts': {\n",
    "            'mean': float(np.mean(classical_results['gdt_ts'])),\n",
    "            'std': float(np.std(classical_results['gdt_ts'])),\n",
    "            'median': float(np.median(classical_results['gdt_ts']))\n",
    "        }\n",
    "    },\n",
    "    'statistical_tests': stat_results\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(RESULTS_DIR, 'RESULTS_SUMMARY.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f'‚úÖ Summary report saved to: {summary_path}')\n",
    "\n",
    "# Print summary\n",
    "print('\\n' + '=' * 80)\n",
    "print('EXPERIMENT SUMMARY')\n",
    "print('=' * 80)\n",
    "print(json.dumps(summary, indent=2))\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_archive"
   },
   "outputs": [],
   "source": [
    "# Create downloadable archive\n",
    "import shutil\n",
    "\n",
    "archive_name = f'quantumfold_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "archive_path = shutil.make_archive(\n",
    "    os.path.join('/content', archive_name),\n",
    "    'zip',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "archive_size = os.path.getsize(archive_path) / 1e6\n",
    "\n",
    "print(f'‚úÖ Results archive created: {archive_path}')\n",
    "print(f'   Size: {archive_size:.1f}MB')\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import files\n",
    "    print('\\nüì• Download archive:')\n",
    "    files.download(archive_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## ‚úÖ Benchmark Complete!\n",
    "\n",
    "### What You Now Have:\n",
    "\n",
    "1. **Trained Models**\n",
    "   - Quantum-enhanced model checkpoint\n",
    "   - Classical baseline checkpoint\n",
    "   - Training histories\n",
    "\n",
    "2. **Performance Metrics**\n",
    "   - TM-score, RMSD, GDT-TS for both models\n",
    "   - Raw results CSV\n",
    "   - Statistical analysis\n",
    "\n",
    "3. **Publication Figures**\n",
    "   - Training curves\n",
    "   - Metric distributions\n",
    "   - Paired comparisons\n",
    "\n",
    "4. **Complete Documentation**\n",
    "   - Experiment configuration\n",
    "   - Results summary\n",
    "   - Statistical validation\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Review the statistical significance of results\n",
    "2. Analyze the plots and distributions\n",
    "3. Download the archive for local analysis\n",
    "4. Use the results for your publication/presentation\n",
    "\n",
    "### Citation:\n",
    "\n",
    "```bibtex\n",
    "@software{quantumfold2026,\n",
    "  author = {Marena, Tommaso R.},\n",
    "  title = {QuantumFold-Advantage: Quantum-Classical Hybrid Architecture for Protein Structure Prediction},\n",
    "  year = {2026},\n",
    "  institution = {The Catholic University of America},\n",
    "  url = {https://github.com/Tommaso-R-Marena/QuantumFold-Advantage}\n",
    "}\n",
    "```\n",
    "\n",
    "**‚≠ê If you found this useful, please star the repository!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
