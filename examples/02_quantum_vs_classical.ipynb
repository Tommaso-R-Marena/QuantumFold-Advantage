{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Quantum vs Classical Model Comparison\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/02_quantum_vs_classical.ipynb)\n",
    "\n",
    "This tutorial demonstrates how to train and compare quantum-enhanced and classical models for protein folding.\n",
    "\n",
    "## Topics Covered\n",
    "1. Installation and setup\n",
    "2. Data preparation\n",
    "3. Training quantum models\n",
    "4. Training classical baselines\n",
    "5. Performance benchmarking\n",
    "6. Statistical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## üîß Step 1: Installation\n",
    "\n",
    "Clone repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_environment"
   },
   "outputs": [],
   "source": [
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running locally\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing QuantumFold-Advantage...\")\n",
    "    !git clone https://github.com/Tommaso-R-Marena/QuantumFold-Advantage.git\n",
    "    %cd QuantumFold-Advantage\n",
    "    !pip install -q pennylane matplotlib pandas scikit-learn biopython requests tqdm\n",
    "    print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports_header"
   },
   "source": [
    "## üì¶ Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "if IN_COLAB:\n",
    "    sys.path.insert(0, '/content/QuantumFold-Advantage')\n",
    "else:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.quantum_layers import QuantumAttentionLayer\n",
    "from src.benchmarks import calculate_rmsd, calculate_tm_score\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "seq_length = 50\n",
    "feature_dim = 64\n",
    "\n",
    "# Create random features and targets\n",
    "X_train = torch.randn(n_samples, seq_length, feature_dim)\n",
    "y_train = torch.randn(n_samples, seq_length, 3)  # 3D coordinates\n",
    "\n",
    "X_test = torch.randn(20, seq_length, feature_dim)\n",
    "y_test = torch.randn(20, seq_length, 3)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 4: Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Quantum model\n",
    "class QuantumModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quantum = QuantumAttentionLayer(4, 2, feature_dim)\n",
    "        self.output = nn.Linear(feature_dim, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quantum(x)\n",
    "        return self.output(x)\n",
    "\n",
    "# Classical model\n",
    "class ClassicalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(feature_dim, 4, batch_first=True)\n",
    "        self.output = nn.Linear(feature_dim, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        return self.output(x)\n",
    "\n",
    "quantum_model = QuantumModel().to(device)\n",
    "classical_model = ClassicalModel().to(device)\n",
    "\n",
    "print(f\"Quantum parameters: {sum(p.numel() for p in quantum_model.parameters())}\")\n",
    "print(f\"Classical parameters: {sum(p.numel() for p in classical_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 5: Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, epochs=10, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    losses = []\n",
    "    times = []\n",
    "    \n",
    "    X, y = X.to(device), y.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        losses.append(loss.item())\n",
    "        times.append(epoch_time)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return losses, times\n",
    "\n",
    "# Train quantum model\n",
    "print(\"\\nüî¨ Training Quantum Model...\")\n",
    "q_losses, q_times = train_model(quantum_model, X_train, y_train, epochs=10)\n",
    "\n",
    "# Train classical model\n",
    "print(\"\\nüíª Training Classical Model...\")\n",
    "c_losses, c_times = train_model(classical_model, X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 6: Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss comparison\n",
    "ax1.plot(q_losses, 'b-', label='Quantum', linewidth=2)\n",
    "ax1.plot(c_losses, 'r-', label='Classical', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Time comparison\n",
    "ax2.plot(np.cumsum(q_times), 'b-', label='Quantum', linewidth=2)\n",
    "ax2.plot(np.cumsum(c_times), 'r-', label='Classical', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Time (s)', fontsize=12)\n",
    "ax2.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìà Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Quantum  - Final Loss: {q_losses[-1]:.4f} | Total Time: {sum(q_times):.2f}s\")\n",
    "print(f\"Classical - Final Loss: {c_losses[-1]:.4f} | Total Time: {sum(c_times):.2f}s\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "1. ‚úÖ Set up training pipeline\n",
    "2. ‚úÖ Trained quantum model with variational circuits\n",
    "3. ‚úÖ Trained classical baseline with attention\n",
    "4. ‚úÖ Compared training dynamics\n",
    "5. ‚úÖ Analyzed performance metrics\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- Quantum models may show different convergence patterns\n",
    "- Training time depends on circuit depth and qubit count\n",
    "- Both approaches can achieve competitive performance\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "Explore more:\n",
    "- [Advanced Visualization](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/03_advanced_visualization.ipynb)\n",
    "- [Getting Started](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/01_getting_started.ipynb)\n",
    "\n",
    "‚≠ê **Star the repo if this was helpful!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}