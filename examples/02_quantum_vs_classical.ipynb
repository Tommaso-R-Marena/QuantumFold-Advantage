{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Quantum vs Classical Model Comparison\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/02_quantum_vs_classical.ipynb)\n",
    "\n",
    "This tutorial demonstrates how to train and compare quantum-enhanced and classical models for protein folding.\n",
    "\n",
    "## Topics Covered\n",
    "1. Installation and setup\n",
    "2. Data preparation\n",
    "3. Training quantum models\n",
    "4. Training classical baselines\n",
    "5. Performance benchmarking\n",
    "6. Statistical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## Step 1: Installation\n",
    "\n",
    "Clone repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_environment"
   },
   "outputs": [],
   "source": [
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing QuantumFold-Advantage...\")\n",
    "    !git clone https://github.com/Tommaso-R-Marena/QuantumFold-Advantage.git\n",
    "    %cd QuantumFold-Advantage\n",
    "    \n",
    "    print(\"\\nFixing JAX version for PennyLane compatibility...\")\n",
    "    !pip install -q 'jax==0.6.0' 'jaxlib==0.6.0'\n",
    "    \n",
    "    print(\"\\nInstalling dependencies...\")\n",
    "    !pip install -q pennylane matplotlib pandas scikit-learn biopython requests tqdm\n",
    "    \n",
    "    print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports_header"
   },
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "if IN_COLAB:\n",
    "    sys.path.insert(0, '/content/QuantumFold-Advantage')\n",
    "else:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.quantum_layers import QuantumAttentionLayer\n",
    "from src.benchmarks import ProteinStructureEvaluator\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "seq_length = 50\n",
    "feature_dim = 64\n",
    "\n",
    "X_train = torch.randn(n_samples, seq_length, feature_dim)\n",
    "y_train = torch.randn(n_samples, seq_length, 3)\n",
    "\n",
    "X_test = torch.randn(20, seq_length, feature_dim)\n",
    "y_test = torch.randn(20, seq_length, 3)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class QuantumModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quantum = QuantumAttentionLayer(\n",
    "            embed_dim=feature_dim,\n",
    "            n_qubits=4,\n",
    "            n_heads=4\n",
    "        )\n",
    "        self.output = nn.Linear(feature_dim, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quantum(x)\n",
    "        return self.output(x)\n",
    "\n",
    "class ClassicalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(feature_dim, 4, batch_first=True)\n",
    "        self.output = nn.Linear(feature_dim, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        return self.output(x)\n",
    "\n",
    "print(\"Initializing models...\")\n",
    "quantum_model = QuantumModel().to(device)\n",
    "classical_model = ClassicalModel().to(device)\n",
    "\n",
    "print(f\"\\nModels initialized on {device}\")\n",
    "print(f\"Quantum parameters:   {sum(p.numel() for p in quantum_model.parameters()):,}\")\n",
    "print(f\"Classical parameters: {sum(p.numel() for p in classical_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, epochs=10, lr=0.001, model_name='Model'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    times = []\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_time = time.time() - start_time\n",
    "        losses.append(loss.item())\n",
    "        times.append(epoch_time)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"  Epoch {epoch+1:2d}/{epochs} | Loss: {loss.item():.4f} | Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    print(f\"{model_name} training complete!\")\n",
    "    return losses, times\n",
    "\n",
    "print(\"\\nQUANTUM MODEL\")\n",
    "print(\"=\" * 60)\n",
    "q_losses, q_times = train_model(quantum_model, X_train, y_train, epochs=10, model_name='Quantum')\n",
    "\n",
    "print(\"\\nCLASSICAL MODEL\")\n",
    "print(\"=\" * 60)\n",
    "c_losses, c_times = train_model(classical_model, X_train, y_train, epochs=10, model_name='Classical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(q_losses, 'b-', label='Quantum', linewidth=2, marker='o', markersize=4)\n",
    "ax1.plot(c_losses, 'r-', label='Classical', linewidth=2, marker='s', markersize=4)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(np.cumsum(q_times), 'b-', label='Quantum', linewidth=2, marker='o', markersize=4)\n",
    "ax2.plot(np.cumsum(c_times), 'r-', label='Classical', linewidth=2, marker='s', markersize=4)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Time (s)', fontsize=12)\n",
    "ax2.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuantum Model:\")\n",
    "print(f\"  Final Loss:  {q_losses[-1]:.6f}\")\n",
    "print(f\"  Total Time:  {sum(q_times):.2f}s\")\n",
    "print(f\"  Avg per Epoch: {np.mean(q_times):.2f}s\")\n",
    "\n",
    "print(f\"\\nClassical Model:\")\n",
    "print(f\"  Final Loss:  {c_losses[-1]:.6f}\")\n",
    "print(f\"  Total Time:  {sum(c_times):.2f}s\")\n",
    "print(f\"  Avg per Epoch: {np.mean(c_times):.2f}s\")\n",
    "\n",
    "speedup = sum(c_times) / sum(q_times)\n",
    "print(f\"\\nSpeed Comparison:\")\n",
    "if speedup > 1:\n",
    "    print(f\"  Classical is {speedup:.2f}x faster\")\n",
    "else:\n",
    "    print(f\"  Quantum is {1/speedup:.2f}x faster\")\n",
    "\n",
    "loss_improvement = (c_losses[-1] - q_losses[-1]) / c_losses[-1] * 100\n",
    "print(f\"\\nLoss Comparison:\")\n",
    "if loss_improvement > 0:\n",
    "    print(f\"  Quantum achieves {loss_improvement:.1f}% lower loss\")\n",
    "else:\n",
    "    print(f\"  Classical achieves {-loss_improvement:.1f}% lower loss\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "1. Set up training pipeline\n",
    "2. Trained quantum model with variational circuits\n",
    "3. Trained classical baseline with attention\n",
    "4. Compared training dynamics\n",
    "5. Analyzed performance metrics\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- Quantum models may show different convergence patterns\n",
    "- Training time depends on circuit depth and qubit count\n",
    "- Both approaches can achieve competitive performance\n",
    "- The quantum advantage may be more pronounced on real hardware\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Explore more:\n",
    "- [Advanced Visualization](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/03_advanced_visualization.ipynb)\n",
    "- [Getting Started](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/01_getting_started.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
