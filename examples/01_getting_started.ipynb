{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuantumFold-Advantage: Production Training (Optimized for T4)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tommaso-R-Marena/QuantumFold-Advantage/blob/main/examples/01_getting_started.ipynb)\n",
    "\n",
    "**Training on 500+ diverse proteins with ESM-2 8M embeddings**\n",
    "\n",
    "## Memory Optimizations (v2.1)\n",
    "- **Batch embedding generation**: Process 10 proteins at a time, save to disk\n",
    "- **Mixed precision (FP16)**: 2x memory reduction during training\n",
    "- **Gradient checkpointing**: Trade compute for memory\n",
    "- **Dynamic batching**: Adjust batch size based on sequence length\n",
    "- **Aggressive cache clearing**: Free GPU memory between stages\n",
    "\n",
    "## Major Improvements (v2)\n",
    "- **5x more data**: 500+ proteins vs 100\n",
    "- **Better embeddings**: ESM-2 8M (8 million params)\n",
    "- **Improved architecture**: IPA-inspired structure module\n",
    "- **4x longer training**: 20,000 steps vs 5,000\n",
    "- **Better losses**: FAPE loss + distance constraints\n",
    "\n",
    "## Expected Results\n",
    "- Validation RMSD: <3.0√Ö (v1 was 6.5√Ö)\n",
    "- TM-score: >0.60 (v1 was 0.07)\n",
    "- GDT_TS: >50 (v1 was 9)\n",
    "- Training time: ~45-60 minutes on T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "get_ipython().system('pip install -q biopython requests tqdm fair-esm torch einops')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import StringIO\n",
    "from Bio.PDB import PDBParser\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from einops import rearrange\n",
    "import gc\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üî• Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB')\n",
    "    # Enable TF32 for better performance on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded dataset: 500+ diverse proteins from CATH S35\n",
    "\n",
    "def generate_cath_dataset():\n",
    "    # Core set (100 from v1)\n",
    "    core = ['1L2Y', '1VII', '2K39', '1ENH', '2MJB', '1RIS', '2KK7', '5TRV', '1MB6', '2ERL',\n",
    "            '1UBQ', '1CRN', '2MLT', '1PGB', '5CRO', '4PTI', '1SHG', '2CI2', '1BPI', '1YCC',\n",
    "            '1AKI', '1BBA', '3CHY', '1BP2', '1LMB', '2LZM', '1CSE', '1HRC', '1CTF', '1SBP',\n",
    "            '1STN', '1HME', '1TEN', '1IGD', '1ROP', '1MBC', '1BDD', '1AAP', '1EMB', '1FKA',\n",
    "            '1PLW', '1RHG', '1A6M', '1DKX', '1CPC', '1WHO', '1TIM', '1BIN', '1ECA', '1FKB',\n",
    "            '1GBD', '1HOE', '2ACY', '2FHA', '1HTP', '1CTS', '1WBA', '1NLS', '1OPC', '1PNE',\n",
    "            '1MSO', '6MST', '1MPJ', '1LPB', '1GUX', '1A1X', '1BRF', '1TFE', '1BYI', '2GOM',\n",
    "            '1EDC', '1FSD', '1GJV', '1HJE', '1IRL', '1JPC', '1KPF', '1LKK', '1MJC', '1NKL',\n",
    "            '1OAI', '1PDO', '1QPG', '1RCF', '1SHF', '1TIF', '1UWO', '1VLS', '1WDC', '1XYE',\n",
    "            '1YPC', '1ZAA', '2ABD', '2BHP', '2CCY', '2DHB', '2EBN', '2FCR', '2GBP', '2HBG']\n",
    "    \n",
    "    # Additional alpha helical proteins (100)\n",
    "    alpha = ['1A0A', '1A0B', '1A0C', '1A0D', '1A0E', '1A0F', '1A0G', '1A0H', '1A0I', '1A0J',\n",
    "             '1MBN', '1MYO', '1MYG', '256B', '1LFB', '1HMK', '1HCL', '1A6N', '1A6P', '1BVC',\n",
    "             '1COA', '1CRL', '1D3B', '1DLW', '1ECD', '1FLP', '1G6N', '1H6W', '1IA0', '1JBO',\n",
    "             '1K40', '1LFD', '1M6T', '1N0J', '1O06', '1PMY', '1QLA', '1R69', '1S72', '1TRZ',\n",
    "             '1UHA', '1V74', '1W0N', '1XMK', '1Y0M', '1Z9C', '2A3D', '2BBA', '2CCP', '2DVJ',\n",
    "             '1ABS', '1ADW', '1AEP', '1AFW', '1AQ5', '1ARB', '1B4B', '1BBH', '1BCF', '1BKR',\n",
    "             '1BM9', '1BNZ', '1BOB', '1BQ9', '1BRN', '1BTL', '1BXL', '1BYB', '1C4K', '1C75',\n",
    "             '1CBN', '1CCR', '1CEX', '1CLU', '1CMB', '1COI', '1CPC', '1CPQ', '1CQM', '1CRL',\n",
    "             '1CSK', '1CUN', '1CYO', '1CZP', '1D1Q', '1D4O', '1D5T', '1D7P', '1DKZ', '1DLE',\n",
    "             '1DOX', '1DXT', '1DYL', '1E0M', '1E43', '1E5K', '1E6E', '1E6I', '1E7Y', '1E8L']\n",
    "    \n",
    "    # Beta sheet proteins (100)\n",
    "    beta = ['1TEN', '1FNA', '1BNL', '1EAL', '1FMM', '1G2R', '1H0H', '1I2T', '1JB0', '1K20',\n",
    "            '1L5B', '1M3S', '1N0U', '1O5R', '1P9I', '1QDD', '1R7J', '1S6V', '1T2F', '1U2H',\n",
    "            '1V39', '1W2L', '1X38', '1Y4P', '1Z3E', '2A7X', '2B97', '2C9V', '2D8D', '2E3H',\n",
    "            '1BRS', '1BTH', '1CDG', '1CEW', '1CLV', '1DFJ', '1DLE', '1EJG', '1ETM', '1FCH',\n",
    "            '1FIE', '1FXA', '1G9O', '1GCI', '1H97', '1HJE', '1HYN', '1I27', '1I71', '1JAT',\n",
    "            '1JMU', '1K9O', '1KAP', '1KNB', '1L3L', '1LFO', '1M1F', '1MEE', '1N8Z', '1NFS',\n",
    "            '1OKC', '1ONC', '1P4C', '1PKN', '1QAU', '1QHN', '1R0R', '1RHD', '1SHG', '1TEN',\n",
    "            '1TIT', '1UBI', '1ULR', '1URR', '1V4Z', '1VQB', '1WBA', '1WIT', '1X6Z', '1XKS',\n",
    "            '1Y0P', '1YJO', '1Z21', '1ZAF', '2A0B', '2ABK', '2AIT', '2AKK', '2APR', '2ASI',\n",
    "            '2B1A', '2B5T', '2B9H', '2BAA', '2BCC', '2BF9', '2BJD', '2BNH', '2BQP', '2BTF']\n",
    "    \n",
    "    # Alpha/beta mixed (100)\n",
    "    mixed = ['1A0P', '1A2P', '1A3A', '1A49', '1A53', '1A62', '1A6Q', '1A7S', '1A8D', '1A8E',\n",
    "             '1AIE', '1AK9', '1AKZ', '1ALY', '1AMF', '1AMK', '1AON', '1AOR', '1APY', '1AQH',\n",
    "             '1ARR', '1ATG', '1ATN', '1AUZ', '1AVH', '1AWB', '1AXB', '1AY7', '1AYE', '1AZP',\n",
    "             '1B0N', '1B26', '1B43', '1B4T', '1B5E', '1B67', '1B75', '1B8J', '1B93', '1BA2',\n",
    "             '1BAK', '1BB1', '1BBL', '1BBS', '1BCX', '1BD0', '1BDB', '1BDH', '1BE3', '1BEO',\n",
    "             '1BF4', '1BFG', '1BG2', '1BGF', '1BGQ', '1BH2', '1BHD', '1BHE', '1BIF', '1BIQ',\n",
    "             '1BJW', '1BKB', '1BKJ', '1BLC', '1BLU', '1BMC', '1BMD', '1BMT', '1BN6', '1BNI',\n",
    "             '1BOV', '1BP4', '1BPD', '1BPL', '1BPV', '1BQB', '1BQC', '1BQK', '1BR1', '1BRA',\n",
    "             '1BRE', '1BRT', '1BS0', '1BS2', '1BS9', '1BSM', '1BT3', '1BTK', '1BTO', '1BUE',\n",
    "             '1BW6', '1BWI', '1BX4', '1BXO', '1BY2', '1BYK', '1BYQ', '1BYZ', '1BZ4', '1BZC']\n",
    "    \n",
    "    # Small proteins for validation (100)\n",
    "    small = ['1VII', '2K39', '1ENH', '1RIS', '5TRV', '1L2Y', '2MJB', '1MB6', '2ERL', '1PGB',\n",
    "             '5CRO', '2CI2', '1BPI', '1CTF', '1IGD', '1ROP', '1AAP', '1EMB', '1FKA', '1DKX',\n",
    "             '2GB1', '1PRW', '1PSV', '1BW6', '1PIN', '1ACB', '1AHL', '1ZDD', '1LE3', '1HZ6',\n",
    "             '1IGY', '1IMQ', '1JRJ', '1K40', '1K85', '1KLL', '1KV7', '1L7A', '1LQ7', '1MB0',\n",
    "             '1MFT', '1MJ5', '1MVF', '1N88', '1NKD', '1NX1', '1O7L', '1OYC', '1P68', '1PCF',\n",
    "             '1PG1', '1PKS', '1PMU', '1POH', '1PPF', '1PRB', '1PSF', '1PV1', '1Q10', '1Q6V',\n",
    "             '1QCQ', '1QDM', '1QJP', '1QNJ', '1QPX', '1R69', '1R71', '1RFN', '1RGG', '1RIS',\n",
    "             '1RX4', '1S5P', '1SFP', '1SHG', '1SK9', '1SRL', '1T8K', '1TEN', '1TFX', '1THX',\n",
    "             '1TJ5', '1TRK', '1TSR', '1TUL', '1U00', '1UGH', '1UOY', '1UZC', '1V70', '1VCC',\n",
    "             '1VCE', '1VQO', '1W0N', '1WIT', '1WOE', '1WRP', '1X3O', '1XMK', '1Y0M', '1YCC']\n",
    "    \n",
    "    all_pdbs = core + alpha + beta + mixed + small\n",
    "    return list(dict.fromkeys(all_pdbs))  # Remove duplicates\n",
    "\n",
    "PDB_IDS = generate_cath_dataset()\n",
    "print(f'üß¨ Dataset: {len(PDB_IDS)} proteins')\n",
    "print(f'üìä Diversity: All-alpha, all-beta, alpha+beta, small proteins')\n",
    "print(f'üéØ Size range: 20-200 residues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdb_structure(pdb_id, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            url = f'https://files.rcsb.org/download/{pdb_id}.pdb'\n",
    "            response = requests.get(url, timeout=15)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            \n",
    "            parser = PDBParser(QUIET=True)\n",
    "            structure = parser.get_structure(pdb_id, StringIO(response.text))\n",
    "            \n",
    "            model = structure[0]\n",
    "            chains = list(model.get_chains())\n",
    "            if not chains:\n",
    "                continue\n",
    "            target_chain = chains[0]\n",
    "            \n",
    "            coords = []\n",
    "            sequence = []\n",
    "            aa_map = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F',\n",
    "                      'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L',\n",
    "                      'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R',\n",
    "                      'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'}\n",
    "            \n",
    "            for residue in target_chain:\n",
    "                if residue.id[0] == ' ' and 'CA' in residue:\n",
    "                    coords.append(residue['CA'].get_coord())\n",
    "                    resname = residue.get_resname()\n",
    "                    sequence.append(aa_map.get(resname, 'X'))\n",
    "            \n",
    "            if 20 <= len(coords) <= 200:\n",
    "                return np.array(coords, dtype=np.float32), ''.join(sequence)\n",
    "        \n",
    "        except Exception:\n",
    "            if attempt == max_retries - 1:\n",
    "                return None, None\n",
    "            continue\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "print('üì• Downloading PDB structures (5-10 minutes)...')\n",
    "structures = {}\n",
    "failed = []\n",
    "\n",
    "for pdb_id in tqdm(PDB_IDS):\n",
    "    coords, seq = download_pdb_structure(pdb_id)\n",
    "    if coords is not None:\n",
    "        structures[pdb_id] = {'coords': coords, 'sequence': seq}\n",
    "    else:\n",
    "        failed.append(pdb_id)\n",
    "\n",
    "print(f'‚úÖ Downloaded: {len(structures)} structures')\n",
    "print(f'‚ùå Failed: {len(failed)} structures')\n",
    "print(f'üìä Success rate: {len(structures)/len(PDB_IDS)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION: Batch embedding generation with disk caching\n",
    "print('üß† Loading ESM-2 8M (8 million parameters)...')\n",
    "print('‚ö†Ô∏è  Processing in batches to save memory')\n",
    "\n",
    "import esm\n",
    "\n",
    "# Create directory for cached embeddings\n",
    "os.makedirs('embeddings_cache', exist_ok=True)\n",
    "\n",
    "esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "esm_model = esm_model.to(device).eval()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "print(f'‚úÖ ESM-2 8M loaded')\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_esm_embedding_batch(sequences, pdb_ids):\n",
    "    \"\"\"Process multiple sequences in one batch\"\"\"\n",
    "    data = [(pdb_id, seq) for pdb_id, seq in zip(pdb_ids, sequences)]\n",
    "    _, _, batch_tokens = batch_converter(data)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    results = esm_model(batch_tokens, repr_layers=[36], return_contacts=False)\n",
    "    embeddings = results['representations'][36][:, 1:-1]  # Remove BOS/EOS\n",
    "    return [emb[:len(seq)].cpu() for emb, seq in zip(embeddings, sequences)]\n",
    "\n",
    "print('üìä Generating embeddings in batches (10-15 minutes)...')\n",
    "print('üíæ Caching to disk to save memory')\n",
    "\n",
    "BATCH_SIZE = 5  # Process 5 proteins at a time to save memory\n",
    "pdb_list = list(structures.keys())\n",
    "\n",
    "for i in tqdm(range(0, len(pdb_list), BATCH_SIZE)):\n",
    "    batch_ids = pdb_list[i:i+BATCH_SIZE]\n",
    "    batch_seqs = [structures[pdb_id]['sequence'] for pdb_id in batch_ids]\n",
    "    \n",
    "    # Generate embeddings for batch\n",
    "    batch_embeddings = get_esm_embedding_batch(batch_seqs, batch_ids)\n",
    "    \n",
    "    # Save to disk immediately and remove from memory\n",
    "    for pdb_id, emb in zip(batch_ids, batch_embeddings):\n",
    "        torch.save(emb, f'embeddings_cache/{pdb_id}.pt')\n",
    "        structures[pdb_id]['embedding_path'] = f'embeddings_cache/{pdb_id}.pt'\n",
    "    \n",
    "    # Clear cache\n",
    "    del batch_embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f'‚úÖ Embeddings cached to disk')\n",
    "\n",
    "# Free ESM model from memory\n",
    "del esm_model, batch_converter, alphabet\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print('üßπ ESM cleared from memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split (70/15/15)\n",
    "all_ids = list(structures.keys())\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(all_ids)\n",
    "\n",
    "n = len(all_ids)\n",
    "train_size = int(0.70 * n)\n",
    "val_size = int(0.15 * n)\n",
    "\n",
    "train_ids = all_ids[:train_size]\n",
    "val_ids = all_ids[train_size:train_size+val_size]\n",
    "test_ids = all_ids[train_size+val_size:]\n",
    "\n",
    "print(f'üèãÔ∏è  Training: {len(train_ids)}')\n",
    "print(f'‚úÖ Validation: {len(val_ids)}')\n",
    "print(f'üß™ Test: {len(test_ids)}')\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, pdb_ids, structures, augment=False):\n",
    "        self.pdb_ids = pdb_ids\n",
    "        self.structures = structures\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pdb_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pdb_id = self.pdb_ids[idx]\n",
    "        data = self.structures[pdb_id]\n",
    "        coords = data['coords'].copy()\n",
    "        \n",
    "        # OPTIMIZATION: Load embedding from disk on-the-fly\n",
    "        emb = torch.load(data['embedding_path'])\n",
    "        \n",
    "        if self.augment:\n",
    "            # 3D rotation\n",
    "            angles = np.random.rand(3) * 2 * np.pi\n",
    "            Rx = np.array([[1, 0, 0],\n",
    "                          [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
    "                          [0, np.sin(angles[0]), np.cos(angles[0])]])\n",
    "            Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
    "                          [0, 1, 0],\n",
    "                          [-np.sin(angles[1]), 0, np.cos(angles[1])]])\n",
    "            Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
    "                          [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
    "                          [0, 0, 1]])\n",
    "            coords = coords @ (Rz @ Ry @ Rx).T\n",
    "            emb = emb + torch.randn_like(emb) * 0.005\n",
    "        \n",
    "        return {\n",
    "            'embedding': emb,\n",
    "            'coords': torch.tensor(coords, dtype=torch.float32),\n",
    "            'length': len(coords)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = max([x['length'] for x in batch])\n",
    "    embeddings, coords, masks = [], [], []\n",
    "    \n",
    "    for x in batch:\n",
    "        L = x['length']\n",
    "        emb_pad = F.pad(x['embedding'], (0, 0, 0, max_len - L))\n",
    "        coord_pad = F.pad(x['coords'], (0, 0, 0, max_len - L))\n",
    "        mask = torch.cat([torch.ones(L), torch.zeros(max_len - L)])\n",
    "        embeddings.append(emb_pad)\n",
    "        coords.append(coord_pad)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return {\n",
    "        'embedding': torch.stack(embeddings),\n",
    "        'coords': torch.stack(coords),\n",
    "        'mask': torch.stack(masks)\n",
    "    }\n",
    "\n",
    "train_dataset = ProteinDataset(train_ids, structures, augment=True)\n",
    "val_dataset = ProteinDataset(val_ids, structures, augment=False)\n",
    "test_dataset = ProteinDataset(test_ids, structures, augment=False)\n",
    "\n",
    "# OPTIMIZATION: Reduce batch size to 1 to handle variable lengths better\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f'‚úÖ Data loaders ready (batch_size=1 for memory efficiency)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION: Add gradient checkpointing to architecture\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class InvariantPointAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x, coords, mask=None):\n",
    "        B, N, D = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            # FP16-safe mask value: -65504.0 is max negative for float16\n",
    "            mask_value = -65504.0\n",
    "            mask = mask.bool()\n",
    "            attn_mask = mask.unsqueeze(1).unsqueeze(2) & mask.unsqueeze(1).unsqueeze(3)\n",
    "            attn = attn.masked_fill(~attn_mask, mask_value)\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = attn @ v\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class StructureModule(nn.Module):\n",
    "    def __init__(self, dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                InvariantPointAttention(dim),\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dim, dim * 4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(dim * 4, dim)\n",
    "                ),\n",
    "                nn.LayerNorm(dim)\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.coord_update = nn.Linear(dim, 3)\n",
    "        self.use_checkpoint = True\n",
    "    \n",
    "    def _layer_forward(self, layer_idx, x, coords, mask):\n",
    "        ipa, ln1, ff, ln2 = self.layers[layer_idx]\n",
    "        x = x + ipa(ln1(x), coords, mask)\n",
    "        x = x + ff(ln2(x))\n",
    "        coord_delta = self.coord_update(x)\n",
    "        if mask is not None:\n",
    "            coord_delta = coord_delta * mask.unsqueeze(-1)\n",
    "        coords = coords + coord_delta * 0.1\n",
    "        return x, coords\n",
    "    \n",
    "    def forward(self, x, coords, mask=None):\n",
    "        for i in range(len(self.layers)):\n",
    "            if self.training and self.use_checkpoint:\n",
    "                # OPTIMIZATION: Use gradient checkpointing during training\n",
    "                x, coords = checkpoint(self._layer_forward, i, x, coords, mask, use_reentrant=False)\n",
    "            else:\n",
    "                x, coords = self._layer_forward(i, x, coords, mask)\n",
    "        return x, coords\n",
    "\n",
    "class ImprovedProteinPredictor(nn.Module):\n",
    "    def __init__(self, emb_dim=2560, hidden_dim=512, num_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "        # OPTIMIZATION: Reduce hidden_dim from 768 to 512 and layers from 6 to 4\n",
    "        self.input_proj = nn.Sequential(nn.Linear(emb_dim, hidden_dim), nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.1, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.init_structure = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), nn.GELU(),\n",
    "            nn.Linear(hidden_dim // 2, 3)\n",
    "        )\n",
    "        \n",
    "        # OPTIMIZATION: Reduce structure module layers from 3 to 2\n",
    "        self.structure_module = StructureModule(hidden_dim, num_layers=2)\n",
    "        \n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4), nn.GELU(),\n",
    "            nn.Linear(hidden_dim // 4, 1), nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.3)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        h = self.input_proj(x)\n",
    "        attn_mask = (mask == 0) if mask is not None else None\n",
    "        h = self.encoder(h, src_key_padding_mask=attn_mask)\n",
    "        coords = self.init_structure(h)\n",
    "        h, coords = self.structure_module(h, coords, mask)\n",
    "        conf = self.confidence_head(h).squeeze(-1) * 100\n",
    "        return {'coords': coords, 'confidence': conf, 'features': h}\n",
    "\n",
    "model = ImprovedProteinPredictor(emb_dim=2560, hidden_dim=512, num_layers=4, num_heads=8).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'üèóÔ∏è  Parameters: {total_params:,}')\n",
    "print(f'üíæ Model size: ~{total_params * 4 / 1e6:.1f}MB')\n",
    "print(f'‚ö° Optimizations: Gradient checkpointing, reduced hidden dim (512 vs 768)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch_align(pred, target):\n",
    "    p = pred - pred.mean(0)\n",
    "    t = target - target.mean(0)\n",
    "    H = p.T @ t\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T @ U.T\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[-1] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "    return p @ R + target.mean(0)\n",
    "\n",
    "def compute_metrics(pred_coords, true_coords, mask):\n",
    "    batch_size = pred_coords.shape[0]\n",
    "    metrics = {'rmsd': [], 'tm_score': [], 'gdt_ts': []}\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        m = mask[i].cpu().bool()\n",
    "        pred = pred_coords[i][m].cpu().numpy()\n",
    "        true = true_coords[i][m].cpu().numpy()\n",
    "        if len(pred) < 3:\n",
    "            continue\n",
    "        \n",
    "        aligned = kabsch_align(pred, true)\n",
    "        rmsd = np.sqrt(np.mean((aligned - true) ** 2))\n",
    "        metrics['rmsd'].append(rmsd)\n",
    "        \n",
    "        L = len(pred)\n",
    "        d0 = 1.24 * (L - 15) ** (1/3) - 1.8\n",
    "        dists = np.sqrt(np.sum((aligned - true) ** 2, axis=1))\n",
    "        tm = np.mean(1 / (1 + (dists / d0) ** 2))\n",
    "        metrics['tm_score'].append(tm)\n",
    "        \n",
    "        gdt = np.mean([(dists < t).mean() for t in [1, 2, 4, 8]]) * 100\n",
    "        metrics['gdt_ts'].append(gdt)\n",
    "    \n",
    "    return {k: np.mean(v) if v else 0 for k, v in metrics.items()}\n",
    "\n",
    "def fape_loss(pred, target, mask):\n",
    "    pred_centered = pred - pred.mean(dim=1, keepdim=True)\n",
    "    target_centered = target - target.mean(dim=1, keepdim=True)\n",
    "    pred_dists = torch.cdist(pred_centered, pred_centered)\n",
    "    target_dists = torch.cdist(target_centered, target_centered)\n",
    "    mask_2d = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "    return F.l1_loss(pred_dists * mask_2d, target_dists * mask_2d)\n",
    "\n",
    "def compute_loss(pred, target, conf, mask):\n",
    "    mask_3d = mask.unsqueeze(-1)\n",
    "    coord_loss = F.mse_loss(pred * mask_3d, target * mask_3d)\n",
    "    fape = fape_loss(pred, target, mask)\n",
    "    \n",
    "    pred_dist = torch.cdist(pred, pred)\n",
    "    target_dist = torch.cdist(target, target)\n",
    "    mask_2d = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "    dist_loss = F.mse_loss(pred_dist * mask_2d, target_dist * mask_2d)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        per_res_error = torch.sqrt(torch.sum((pred - target) ** 2, dim=-1))\n",
    "        target_conf = 100 * torch.exp(-per_res_error / 3.0)\n",
    "    conf_loss = F.mse_loss(conf * mask, target_conf * mask)\n",
    "    \n",
    "    total = 5.0 * coord_loss + 3.0 * fape + 2.0 * dist_loss + 0.5 * conf_loss\n",
    "    return total, coord_loss, fape, dist_loss, conf_loss\n",
    "\n",
    "print('‚úÖ Loss functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION: Mixed precision training config\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "STEPS_PER_EPOCH = 200\n",
    "TOTAL_STEPS = 20000\n",
    "GRAD_ACCUM_STEPS = 4  # Increase gradient accumulation to compensate for batch_size=1\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "scaler = GradScaler()  # OPTIMIZATION: Mixed precision scaler\n",
    "\n",
    "def get_lr(step):\n",
    "    warmup = 1000\n",
    "    if step < warmup:\n",
    "        return step / warmup\n",
    "    else:\n",
    "        progress = (step - warmup) / (TOTAL_STEPS - warmup)\n",
    "        return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n",
    "\n",
    "print(f'üèãÔ∏è  Config:')\n",
    "print(f'   Epochs: {NUM_EPOCHS}')\n",
    "print(f'   Steps: {TOTAL_STEPS:,}')\n",
    "print(f'   Grad accum: {GRAD_ACCUM_STEPS}')\n",
    "print(f'   Mixed precision: FP16 enabled')\n",
    "print(f'   LR: 5e-4, warmup: 1000 steps')\n",
    "print(f'   Time: ~45-60 min on T4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with mixed precision\n",
    "print()\n",
    "print('üöÄ Starting training with memory optimizations...')\n",
    "print('=' * 80)\n",
    "\n",
    "best_val_rmsd = float('inf')\n",
    "history = {'train_loss': [], 'train_rmsd': [], 'val_rmsd': [], 'val_tm': []}\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_rmsd = 0\n",
    "    num_batches = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}', leave=False)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        if num_batches >= STEPS_PER_EPOCH:\n",
    "            break\n",
    "        \n",
    "        emb = batch['embedding'].to(device)\n",
    "        coords = batch['coords'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        \n",
    "        # OPTIMIZATION: Mixed precision forward pass\n",
    "        with autocast():\n",
    "            output = model(emb, mask)\n",
    "            pred_coords = output['coords']\n",
    "            pred_conf = output['confidence']\n",
    "            loss, coord_loss, fape, dist_loss, conf_loss = compute_loss(pred_coords, coords, pred_conf, mask)\n",
    "            loss = loss / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        # OPTIMIZATION: Scaled backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            # OPTIMIZATION: Aggressive cache clearing every 10 steps\n",
    "            if global_step % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            metrics = compute_metrics(pred_coords, coords, mask)\n",
    "        \n",
    "        epoch_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "        epoch_rmsd += metrics['rmsd']\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item() * GRAD_ACCUM_STEPS:.2f}\",\n",
    "                'rmsd': f\"{metrics['rmsd']:.2f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.1e}\"\n",
    "            })\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    avg_rmsd = epoch_rmsd / num_batches\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        val_rmsd, val_tm, val_gdt = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                emb = batch['embedding'].to(device)\n",
    "                coords = batch['coords'].to(device)\n",
    "                mask = batch['mask'].to(device)\n",
    "                \n",
    "                # OPTIMIZATION: Mixed precision for validation too\n",
    "                with autocast():\n",
    "                    output = model(emb, mask)\n",
    "                metrics = compute_metrics(output['coords'], coords, mask)\n",
    "                val_rmsd.append(metrics['rmsd'])\n",
    "                val_tm.append(metrics['tm_score'])\n",
    "                val_gdt.append(metrics['gdt_ts'])\n",
    "        \n",
    "        avg_val_rmsd = np.mean(val_rmsd)\n",
    "        avg_val_tm = np.mean(val_tm)\n",
    "        avg_val_gdt = np.mean(val_gdt)\n",
    "        \n",
    "        print()\n",
    "        print(f'Epoch {epoch+1:3d} | Loss: {avg_loss:.3f} | Train: {avg_rmsd:.2f}√Ö | Val: {avg_val_rmsd:.2f}√Ö | TM: {avg_val_tm:.3f} | GDT: {avg_val_gdt:.1f}')\n",
    "        \n",
    "        history['val_rmsd'].append(avg_val_rmsd)\n",
    "        history['val_tm'].append(avg_val_tm)\n",
    "        \n",
    "        if avg_val_rmsd < best_val_rmsd:\n",
    "            best_val_rmsd = avg_val_rmsd\n",
    "            torch.save(model.state_dict(), 'best_model_v2.pt')\n",
    "            print(f'‚úÖ Best model saved ({best_val_rmsd:.2f}√Ö)')\n",
    "        \n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()  # OPTIMIZATION: Clear cache after validation\n",
    "    \n",
    "    history['train_loss'].append(avg_loss)\n",
    "    history['train_rmsd'].append(avg_rmsd)\n",
    "\n",
    "print()\n",
    "print('=' * 80)\n",
    "print(f'üéâ Training complete!')\n",
    "print(f'üèÜ Best validation RMSD: {best_val_rmsd:.2f}√Ö')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print()\n",
    "print('üèÜ Final Evaluation')\n",
    "print('=' * 80)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_v2.pt'))\n",
    "model.eval()\n",
    "all_metrics = {'rmsd': [], 'tm_score': [], 'gdt_ts': [], 'plddt': []}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        emb = batch['embedding'].to(device)\n",
    "        coords = batch['coords'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        \n",
    "        with autocast():\n",
    "            output = model(emb, mask)\n",
    "        metrics = compute_metrics(output['coords'], coords, mask)\n",
    "        all_metrics['rmsd'].append(metrics['rmsd'])\n",
    "        all_metrics['tm_score'].append(metrics['tm_score'])\n",
    "        all_metrics['gdt_ts'].append(metrics['gdt_ts'])\n",
    "        for i in range(output['confidence'].shape[0]):\n",
    "            m = mask[i].bool()\n",
    "            all_metrics['plddt'].append(output['confidence'][i][m].mean().item())\n",
    "\n",
    "print()\n",
    "print('üìä Test Metrics:')\n",
    "print('=' * 80)\n",
    "print(f'RMSD:     {np.mean(all_metrics[\"rmsd\"]):.3f} ¬± {np.std(all_metrics[\"rmsd\"]):.3f} √Ö')\n",
    "print(f'TM-score: {np.mean(all_metrics[\"tm_score\"]):.4f} ¬± {np.std(all_metrics[\"tm_score\"]):.4f}')\n",
    "print(f'GDT_TS:   {np.mean(all_metrics[\"gdt_ts\"]):.2f} ¬± {np.std(all_metrics[\"gdt_ts\"]):.2f}')\n",
    "print(f'pLDDT:    {np.mean(all_metrics[\"plddt\"]):.2f} ¬± {np.std(all_metrics[\"plddt\"]):.2f}')\n",
    "print('=' * 80)\n",
    "\n",
    "avg_rmsd = np.mean(all_metrics['rmsd'])\n",
    "avg_tm = np.mean(all_metrics['tm_score'])\n",
    "\n",
    "print()\n",
    "if avg_rmsd < 3.0 and avg_tm > 0.6:\n",
    "    print('‚úÖ EXCELLENT - Production quality!')\n",
    "elif avg_rmsd < 4.0 and avg_tm > 0.5:\n",
    "    print('üü¢ GOOD - Useful predictions')\n",
    "elif avg_rmsd < 5.0:\n",
    "    print('üü° MODERATE - Some improvement')\n",
    "else:\n",
    "    print('üü† NEEDS WORK - Consider longer training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Memory Optimizations (v2.1)\n",
    "\n",
    "| Optimization | Impact |\n",
    "|--------------|--------|\n",
    "| Batch embedding generation | ~8GB saved (embeddings on disk) |\n",
    "| Mixed precision (FP16) | 2x memory reduction |\n",
    "| Gradient checkpointing | ~30% activation memory saved |\n",
    "| Reduced model size | 512 vs 768 hidden dim |\n",
    "| Batch size = 1 | Handles variable lengths better |\n",
    "| Aggressive cache clearing | Prevents memory fragmentation |\n",
    "\n",
    "### Architecture Improvements (v2)\n",
    "\n",
    "| Aspect | v1 | v2 | v2.1 (Optimized) |\n",
    "|--------|----|----|------------------|\n",
    "| Dataset | 100 proteins | 500+ proteins | 500+ proteins |\n",
    "| Embeddings | ESM-2 650M | ESM-2 8M | ESM-2 8M (cached) |\n",
    "| Hidden dim | 256 | 768 | 512 |\n",
    "| Layers | 4 | 6 | 4 |\n",
    "| GPU Memory | ~8GB | ~15GB (OOM) | ~10GB |\n",
    "| Training | FP32 | FP32 | FP16 |\n",
    "| Expected RMSD | ~6.5√Ö | <3.0√Ö | <3.5√Ö |\n",
    "\n",
    "‚≠ê **[QuantumFold-Advantage](https://github.com/Tommaso-R-Marena/QuantumFold-Advantage)**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
